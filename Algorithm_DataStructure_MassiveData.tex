\documentclass[11pt,reqno]{amsart}
\usepackage{graphicx, url}
\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{titlesec}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\graphicspath{ {./images/} }
\usetikzlibrary{shapes}
\usepgfplotslibrary{polar}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{backgrounds}
\pgfplotsset{every axis/.append style={
                    axis x line=middle,    % put the x axis in the middle
                    axis y line=middle,    % put the y axis in the middle
                    axis line style={<->,color=blue}, % arrows on the axis
                    xlabel={$},          % default put x on x-axis
                    ylabel={$},          % default put y on y-axis
            }}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\usepackage[utf8]{inputenc}

\newtheorem{fact}{Fact}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{remark}[example]{Remark}

\newcommand{\bigsection}[1]{
  \titleformat*{\section}{\centering\LARGE\bfseries}
  \section*{#1}
  \titleformat*{\section}{\large\bfseries} % Reset to the original format
}
\titleformat{\section}
  {\normalfont\Large\bfseries\centering}{\thesection}{1em}{}

\title{Data Structure and Algorithm for Massive Dataset}
\author{MinSeok Song}
\date{}
\pgfplotsset{compat=1.18}
\begin{document}
\maketitle
\section*{Three ways to deal with massive dataset}
\begin{enumerate}
\item Dimensional reduction: the purpose is to minimize the loss of information.
\item Compressed representation: present data in a compact form, but not necessarily predicated on the retainment 
of information. i.e., it may prefer higher compression rates.
\item Interpolation: only use discrete information of the distribution $f$. 
This is useful since we do not have a full function $f$ available. Remember we 
used finite element method in numerical PDE, and the right space of function to discuss
 numerical stability etc was Sobolev space.
\end{enumerate}
\begin{itemize}
\item All in all, it focuses on achieving lower computational/statistical complexity.
\item To clarify, computational complexity deals with the resources(time and space), 
while statistical complexity with the intricacy of models(in the sense of how simpler model represents reduced data). 
\end{itemize}
\begin{theorem} (Johnson-Lindenstrauss Lemma)
Let $Q$ be a finite set of vectors in $\mathbb{R}^d$. Let $\delta\in (0,1)$ and $n$ be large enough integer such that
\begin{equation}
\epsilon = \sqrt{\frac{6\log (2\lvert Q\rvert/\delta)}n}\leq 3
\end{equation}
With probability of at least $1-\delta$ over a choice of a random matrix $W\in\mathbb{R}^{n,d}$
 such that each element of $W$ is distributed normally with zero mean and variance of $1/n$ we have
\begin{equation}
\sup_{x\in\mathbb{Q}}\lvert \frac{\lVert Wx\rVert^2}{\lVert x\rVert^2}-1\rvert < \epsilon
\end{equation}
\end{theorem}
\begin{itemize}
\item We might think that $W$ needs to be closer to identity matrix, but this lemma is all about preserving the distance.
\item Do note that each element of $W$ is generated by $N(0,1/N)$, that $k$ (the count of our vectors) is used here.
\item The proof leans on the following lemma, which uses the concentration property of $\chi^2$.
\end{itemize}
\begin{lemma}\label{lem:concentration}
Fix some $x\in\mathbb{R}^d$. Let $W\in\mathbb{R}^{n,d}$ be a random matrix such that each $W_{i,j}$
 is an independent normal random variable. Then, for every $\epsilon\in (0,3)$ we have
 \begin{equation}
 \mathbb{P}[\lvert \frac{\lVert (1/\sqrt n)Wx\rVert}{\lVert 
    x\rVert}-1 \rvert>\epsilon]\leq 2e^{-\epsilon^2 n/6}
 \end{equation}
\end{lemma}
\begin{itemize}
\item Note that $W:\mathbb{R}^d\to\mathbb{R}^n$, and the result does not depend on d. This suggests
 that we can conduct dimensionality reduction in very high-dimensional spaces without much cost(!).
\item This shows the existence of $T=\frac{1}{\sqrt k}\cdot R$ with
 $R\in \mathbb{R}^{k\times d}$, each element generated by $N(0,1)$, when 
 $k\geq \Omega(\frac {\log{k}}{\epsilon^2})$, where $k$ depends on $\delta$ as well. 
\end{itemize}
\begin{proof}[Proof of Lemma \ref{lem:concentration}]
We can assume, WLOG, that $\lVert x\rVert^2=1$. Do note that $\lVert Wx\rVert^2$ has a $\chi_n^2$ distribution
 by construction, so we may use concentration of $\chi^2$ inequality to get the result.
\end{proof}
\begin{proof}
In order to deal with $\lvert Q\rvert$, use the union bound. We can find appropriate $\epsilon$ afterward.
\end{proof}
\begin{itemize}
\item This says that the random projections do not distort Euclidean distances too much.
\end{itemize}

\subsection*{Efficient PCA}
We aim at solving the problem 
\[
\arg\min_{W\in\mathbb{R}^{n,d}, U\in\mathbb{R}^{d,n}}\sum^m_{i=1}\lVert x_i-UWx_i\rVert^2_2
\]
\begin{itemize}
\item It is then shown that the optimal solution is caculated by computing the eigenvectors of $A=\sum^m_{i=1}x_i^Tx_i=XX^T$. 
This is the right eigenvectors of SVD. Do note that $x_i$ is each column of $X$.
\item This means the complexity is given by $O(d^3+md^2)$
\begin{enumerate}
\item $O(d^3)$ for computing the eigenvectors and eigenvalues of $A=XX^T$.
\item $O(d^2 m)$ for computing the covariance matrix $A$.
\end{enumerate}
\item Instead of using $XX^T$, we can use the eigenvector of $B=X^TX$, that is, 
$A(X^Tu)=\lambda(X^T u)$ where $u$ is an eigenvector of $B$.
\item This comes from the fact that $X X^TX u=\lambda X u$.
\item Do note that $B$ only requires calculating inner products $\langle x_i,x_j\rangle$.
\item This reduces our complexity to $O(m^3+dm^2)$, which is useful when d is very large.
\end{itemize}

\section*{Perturbation theory}
\begin{itemize}
\item Instead of considering $Tx=X^*$, let's shift our focus on $X^* = X+\epsilon$.
\item $X^*$ is $r$-dimensional, and the QR decomposition gives $X^*=U^*z^*$, with 
$X^*$ being orthonormal.
\item Weyl's inequality relates to the singular values of perturbed matrix $X^*$.
\begin{theorem}[Weyl's theorem]\label{Theorem:Weyl}
Let $X^*=X+\epsilon$. Then
\[
\lVert \tilde\Lambda-\Lambda\rVert_2\leq\lVert\epsilon\rVert_2.
\]
where $\Lambda$ and $\tilde\Lambda$ are singular value matrices.
\end{theorem}
\item This says that eigenvalue is stable under perturbation. 
\begin{theorem}[Wedin's theorem]
Let $X=X^*+\epsilon$. Then 
\[
\lVert U_{(r,X)} U_{(r,X)}^T-U_{(r,X^*)} U_{(X^*)}^T\rVert_{F}\leq
\frac{\lVert\epsilon\rVert_{F}}{\sigma_r(X)-\sigma_{r+1}(X)}
\]
\end{theorem}
\item This theorem answers the question: if we slightly perturb our matrix, 
how much does the "important" subspaces (as captured by the dominant singular vectors)
 change? This change is inversely proportional to the gap at $r$'th singular value.
\item In light of QR decomposition, we have $X^* = U^* z^*$ for some 
$U^*$ with dimension $d\times r$ and $z^*$ with dimension $r\times N$. Let $X=X^*+\Delta$. By Wedin's inequality, we have
\[
\lVert U_r U_r^T \lvert x^{(i)}-x^{(j)}\rvert\rVert\leq (1+O(\frac{\lVert 
  \Delta\rVert_2}{\sigma_r(x^*)}))\lVert x^{(i)^*}
  -x^{(j)^*}\rVert_2+O(\lVert\Delta\rVert_2)
\]
\item This gives the bound of the perturbation of $x^{(i)}-x^{(j)}$ in terms 
of $\Delta$ and r'th singular value. 
\item SVD has computational cost $O(dN^2)$ and JL has computational cost 
$O(dkN)=O(\frac{N\log N}{\epsilon^2})$
\item We would still prefer the method in JL lemma.
\item If the data matrix has rank $r$-dimensional, we can only require $O(\frac{\log r}{\epsilon^2})$ (?).
 \item Going forward, in summary, we use the perturbation theory 
 + SVD to quantify the approximately dominant subspace of the matrix, and upgrade(?, wrong)
 JL lemma in reality using Weylâ€™s/Wedin's lemma .
\begin{proof}[Proof of Theorem~\ref{Theorem:Weyl}]
First, assume that the matrix is Hermitian. We have 
\[
\lambda_n(A)=\max_{\lVert x\rVert=1}x^TAx
\]
for symmetric matrix $A$.
 It follows that $\min\limits_{dim(A)n=i+1}\max\limits_{x\in A,
\lVert x\rVert=1}x^TAx$. Using this, we can prove that
\[
\lambda_i(A)+\lambda_j(B)\geq \lambda_{i+j-1}(A+B)
\] where each $\lambda$'s are ordered.
Similarly,
\[
\lambda_i(A)+\lambda_j(B)\leq \lambda_{i+j-n}(A+B)
\]
The first inequality shows that
\[
\lvert \lambda_i(A+B)-\lambda_i(A)\rvert\leq \lVert B\rVert_2
\]
which is equivalent to
\[
  \lvert \lambda_i(X+\epsilon)-\lambda_i(X)\rvert\leq \lVert \epsilon\rVert_2
\]
in the setup of our theorem.
We can generalize this by taking

\[
\begin{pmatrix}
  0 & M \\
  M^* & 0 \\
\end{pmatrix}
\]
which gives 
\[
\lvert \sigma_k(X+\epsilon)-\sigma_k(X)\rvert\leq\sigma_1(\epsilon)
\]
\end{proof}
\end{itemize}
\begin{fact}
If A is a bounded self-adjoint operator on a Hilbert space $\mathbb{H}$, then 
\[
\lVert A\rVert = \sup\limits_{\lVert x\rVert =1}\lvert\langle x, Ax\rangle\rvert
\]
\end{fact}
\begin{proof}
It suffices to show $\leq$. 
By definition, we have 
\[
\lVert A\rVert = \sup\limits_{\lVert x\rVert=1}\lVert Ax\rVert
=\sup\{\lvert\langle y, Ax\rangle\lvert\rvert\lVert x\rVert=1, \lVert
  y\rVert=1\}
\]
Now using polarization formula (choose appropriate $y$ to delete imaginary part), we get,
\[
\vert\langle y,Ax\rangle\lvert^2\leq \frac 14\alpha^2(\lVert x\rVert^2+\lVert y\rVert^2)^2
\]
\end{proof}
\begin{itemize}
\item The polarization identity relates the inner product to the norms of linear combination of vectors. This
 is given by
\[
\langle x, y\rangle = \frac 14(\lVert x+y\rVert^2-\lVert x-y\rVert^2+i\lVert x+iy\rVert^2-i\lVert x-iy\rVert^2)
=\sum\limits^3_{k=0}\lVert x+i^ky\rVert^2.
\]
\end{itemize}

\subsection*{Randomized SVD}
\begin{itemize}
  \item We are interested in finding an appropriate $Q$ for $Y=QQ^T Y$.
  \item We may proceed like this.
  \begin{enumerate}
  \item Multiply random matrix $W$ by $Y$.
  \item Perform QR decomposition, so that $YW=QR$.
  \item Use this Q. Then we have $QQ^TYW=QQ^TQRW=QRW=QRW=YW$
  \end{enumerate}
  \item For performance analysis, Johnson-Lindernstrauss lemma would be helpful.
  \item Some variants to do this faster.
  \begin{enumerate}
  \item Verify if the matrix has certain structures (like Toeplitz).
  \item Process by blocks.
  \item Adaptive methods; start with a small rank and increase it adaptively.
  \item Perform SVD on a smaller matrix.
  \item Power iterations algorithm.
  \end{enumerate}
  \end{itemize}
  
\subsection*{Compressed Sensing}
\begin{itemize}
\item This method juxtaposes PCA method.
\begin{definition}
A matrix $W\in \mathbb{R}^{n,d}$ is $(\epsilon, s)-RIP$ ($\epsilon<1$) if for all $x\neq 0$ s.t. $\lVert x\rVert_0\leq s$
 we have 
 \[
|\frac{\lVert Wx\rVert^2_2}{\lVert x\rVert^2_2}-1|\leq \epsilon
 \]
\end{definition}
\begin{theorem}
If $W\in\mathbb{R}^{n,d}$ is an $(\epsilon, 2s)\sim RIP$ matrix, 
and if $x$ has less than $s$ many nonzero elements, then $x$ is the sparsest vector that
 gets mapped to $Wx$ by the matrix $W$.
\end{theorem}
\begin{proof}
Assume $\tilde x\neq x$. Observe that 
$\lVert x-\tilde x\rVert\leq 2s$ and $W(x-\tilde x)=0$. 
On the other hand, $\lvert 0-1\rvert\leq \epsilon$, contradicting 
the definition of RIP matrix.
\end{proof}
\item This implies that, for specific compression matrices and sparse data, the original data 
can be accurately recovered.
\begin{theorem}
Let $\epsilon<\frac 1{1+\sqrt{2}}$ and let $W$ be a ($\epsilon, 2s$)-RIP matrix. Let
 $x$ be an arbitrary and let $x_s$ be the vector which equals $x$ on the $x$ largest
  elements of $x$ and equals 0 elsewhere. Let $x^*\in \arg\min\limits_{v:Wv=y}\lVert
    v\rVert_1$. Then
\[
\lVert x^*-x\rVert_2\leq 2\frac{1+\rho}{1-\rho}s^{-1/2}\lVert x-x_s\rVert_1
\]
where $\rho=\sqrt{2}\epsilon/(1-\epsilon)$.
\end{theorem}
\begin{remark}
In particular, we have 
\[
x=\arg\min\limits_{v:Wv=y}\lVert
v\rVert_0=\arg\min\limits_{v:Wv=y}\lVert v\rVert_1
\]
for $\lVert x\rVert_0\leq s$.
\end{remark}
\item We used $L^1$ since we are looking for
solutions that are "almost sparse" rather than strictly sparse. Further, 
$L^1$ is convex so we can compute efficiently.
\begin{theorem}
Let $\epsilon, \delta\in(0,1)$.
Let $U$ be orthonormal matrix of size $d\times d$. Further, 
let $W\in\mathbb{R}^{n,d}$ be generated by $N(0,1/n)$ where 
$n\geq 100\frac{s\log(40d/(\delta\epsilon))}{\epsilon^2}$ and $s\in [d]$. Then the matrix
$WU$ is $(\epsilon,s)$-RIP.
\end{theorem}
\begin{remark}
This is useful when the sparsity is hidden, i.e. $y=U\alpha$ where $y$ is sparse.
\end{remark}
\item Connection of PCA with compressed sensing: While PCA identifies the dominant 
subspace in which most of the data's energy or variance lies, compressed sensing 
exploits the fact that signals often have sparse representations in some domain. 
These two concepts are related in the sense that they both exploit inherent structures
in data (low-rank structure and sparsity, respectively) for efficient processing 
or recovery.
\item As another note, we can also exploit the rank of the data matrix.
\end{itemize}

\end{document}