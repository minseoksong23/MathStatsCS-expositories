\documentclass{article}
\usepackage[hyphens,spaces,obeyspaces]{url}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\graphicspath{ {./images/} }
\usetikzlibrary{shapes}
\usepgfplotslibrary{polar}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{backgrounds}
\pgfplotsset{every axis/.append style={
                    axis x line=middle,    % put the x axis in the middle
                    axis y line=middle,    % put the y axis in the middle
                    axis line style={<->,color=blue}, % arrows on the axis
                    xlabel={$},          % default put x on x-axis
                    ylabel={$},          % default put y on y-axis
            }}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\usepackage[utf8]{inputenc}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{remark}[example]{Remark}
\title{On EM, IEEE, numerics, etc}
\author{Min Seok Song}
\date{}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\begin{document}
\maketitle
\begin{definition}
Kullback-Leibler(KL) divergence between p.d.f.s g and f is given by
 $$d_{KL}(g\|f)=E_g[\log(\frac gf)]$$
\end{definition}
This is always nonnegative and it can be shown by Jensen's inequality. Intuitively, 
we have more 'confidence' on g whenever g is greater than f, whence the logarithm is positive.
\begin{definition}
The ELBO (Evidence Lower-Bound) of a p.d.f. g with respect to an unnormalized p.d.f. $\tilde{f}$ is given by

$$ELBO(g):=E_g[\log \frac{\tilde{f}}{g}]$$
\end{definition}
\begin{remark}
\begin{itemize}
\item Simple algebra yields $ELBO(g)\leq \log c$, where c is a normalizing constant.
\item Let's think in Bayesian framework; our unnormalized function in this case is
 $\tilde{f}(\theta):=f(y\vert \theta)f(\theta)$, so $ELBO(g)\leq\log f(y)$.
\item This justifies the name "evidence lower-bound" and this helps with the choice of modeling (essentially maximizing ELBO).
\item We write ELBO(g), but really, what's omitted is that this is with respect to $\tilde f(\theta)$.
\end{itemize}
\end{remark}
\begin{remark}
    \begin{itemize}
\item It follows that $ELBO(g)=E_g[\log f(y\rvert \theta)]-d_{KL}(g(\theta)\| f(\theta))$\\
\item In another Bayesian framework, similar equality (will be used in EM algorithm) is
\begin{align*}
        & ELBO(g,\theta)\\
        & =\int \log(\frac{f(y,z\vert\theta)}{g(z)})g(z)dz\\
        & =-d_{KL}(g(z)\| f(z\vert y,\theta))+\log(f(y\vert\theta))
\end{align*}
\end{itemize}
\end{remark}
The first term promotes matching the data, and the second term promotes matching prior beliefs.
The reason we work with log domain is that, except for obvious reasons, it helps with numerical stability.
\\
Indeed, remark 2 motivates what's called EM algorithm.
\begin{algorithm}
    \caption{EM Algorithm}
    \begin{algorithmic}
    \STATE \textbf{Input:} Initialization of $\theta_0$
    \REPEAT
        \STATE \textbf{E-step:} compute 
        \[
        E_{Z\sim f(z\mid y,\theta_l)}[\log f(y,Z\mid \theta)] = \int \log f(y,z\mid \theta)f(z\mid y,\theta_l)dz
        \]
        \STATE \textbf{M-step:} compute
        \[
        \theta_{l+1} = \text{arg max}_\theta E_{Z\sim f(z\mid y,\theta_l)}[\log f(y,Z\mid \theta)]
        \]
    \UNTIL{some stopping criterion}
    \end{algorithmic}
    \end{algorithm}

\begin{theorem}
We have $\log f(y\vert \theta_l)\leq \log f(y\vert \theta{l+1})$
\end{theorem}
\begin{proof}
Let $g_l(z)=f(z\vert y,\theta_l)$. We have

\begin{align*}
    \log f(y\vert\theta_{l+1}) &= ELBO(g_l,\theta_{l+1})+d_{KL}(g_l| f(z\vert y,\theta_{l+1})) \\
    &\geq ELBO(g_l, \theta_l)+d_{KL}(g_l| f(z\vert y,\theta_l)) \\
    &= \log f(y\vert \theta_l)
    \end{align*}

\end{proof}
\begin{itemize}
\item This shows that likelihood function is non-decreasing for each iteration, and since likelihood is always bounded by 1, we have established the convergence
 of the algorithm.
 \item GMM algorithm is a specific instance of this algorithm. Here, $w_{ik}$ can be thought of as latent variable, 
 corresponding to E-step, and computing $\theta$ and $\pi$ corresponds to M-step.
 \item E-step can be computationally expensive and so we usually use Monte-Carlo method to approximate.
\end{itemize}
\begin{remark}
\begin{itemize}
\item Let us now venture into variational inference, which we use when approximating the intractable distribution. For the choice of possible sets that $f$ admits in $d_{KL}(g\|f)$, we can use \textbf{mean field family}, 
which assumes the independence for coordinate distributions. 
\end{itemize}
\end{remark}
\begin{theorem}
Let $g(x)=\prod^d_{i=1} g_i(x_i)$ with $g_{-i}(x_{-i})$ fixed. Then $$g_i^*(x_i)\propto \exp(E_{g_{-i}}[\log f(x_i, x_{-i}]))$$ maximizes
 $ELBO(g)$.
\end{theorem}
\begin{proof}
By routine algebra (we need to use independence at some point), one can show that $$ELBO(g)=E_{g_i}[\log (\exp(E_{g_{-i}}[\log f(x_i,x_{-i})]))-\log g_i(x_i)]+C$$
and notice that the first term can be phrased as -$d_{KL}(g^*\| g)$, whence $g^*=g$ gives the optimization solution. 
\end{proof}
\begin{remark}
    \begin{itemize}
    \item The intuition is to average out the effect of $x_{-i}$ on log expected value in order to incorporate the independence between $g_i$'s. 
\item This leads to the CAVI algorithm, which approximates the unnormalized target density $\tilde{f}$. After initialization, 
updating $g_i$ will increase ELBO for each $i$, so may iterate until ELBO converges.
\item The disadvantage is that it may be computationally expensive and accuracy might be not so good. 
    \end{itemize}
\end{remark}










\end{document}
