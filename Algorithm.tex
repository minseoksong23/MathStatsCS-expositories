\documentclass{article}
\usepackage[hyphens,spaces,obeyspaces]{url}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage[top=1in, bottom=1in, left=0.75in, right=0.75in]{geometry}
\usetikzlibrary{shapes}
\usepgfplotslibrary{polar}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{backgrounds}
\pgfplotsset{every axis/.append style={
                    axis x line=middle,    % put the x axis in the middle
                    axis y line=middle,    % put the y axis in the middle
                    axis line style={<->,color=blue}, % arrows on the axis
                    xlabel={$},          % default put x on x-axis
                    ylabel={$},          % default put y on y-axis
            }}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\usepackage[utf8]{inputenc}
\usepackage{titlesec}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{fact}{Fact}
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{remark}[example]{Remark}
\newcommand{\bigsection}[1]{
  \titleformat*{\section}{\LARGE\bfseries}
  \section*{#1}
  \titleformat*{\section}{\large\bfseries} % Reset to the original format
}

\title{Algorithms}
\author{MinSeok Song}
\date{}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\begin{document}
\maketitle

\bigsection{Algorithmic paradigm}
\section{Divide and Conquer}
\begin{theorem}[Master method]
    Let \( T(n) = aT\left(\frac{n}{b}\right) + f(n) \). Then:
    \begin{enumerate}
        \item If \( f(n) = O\left(n^{\log_b a - \epsilon}\right) \) for some \( \epsilon > 0 \), then \( T(n) = \Theta(n^{\log_b a}) \).
        \item If 
        \begin{itemize}
            \item \( f(n) = \Theta\left(n^{\log_b a}\right) \), then \( T(n) = \Theta\left(n^{\log_b a} \log n\right) \).
            \item \( f(n) = \Theta\left(n^{\log_b a} \log^k n\right) \), then \( T(n) = \Theta\left(n^{\log_b a} \log^{k+1} n\right) \).
        \end{itemize}
        \item If \( f(n) = \Omega\left(n^{\log_b a + \epsilon}\right) \) for some \( \epsilon > 0 \) and \( af\left(\frac{n}{b}\right) \leq cf(n) \) for some \( c < 1 \), then \( T(n) = \Theta(f(n)) \).
    \end{enumerate}
\end{theorem}    
\begin{itemize}
\item We use substitution method to guess the solution or use induction to prove it formally.
\item Recursion-tree method to discover the right guess (or also can be used for proof).
\item We use Master Method to prove formally.
\item This method doesn't always work: $T(n)=4T(n/2)+n^2/\log n$ (in-between cases of 1 and 2).
\end{itemize}
\begin{example}
\begin{itemize}
\item Merge Sort: $T(n) = 2\cdot T(n/2)+\Theta (n)$.
\item Binary Search: $T(n)=T(n/2)+\Theta(1)$.
\item Insertion Sort: $T(n)=\Theta(n)$ (best case), $T(n)=\Theta(n^2)$ (worst case)
    \begin{itemize}
    \item Iterate from $0$ to $n-1$, swap if in reverse order.
    \end{itemize}
\item Matrix multiplication (Strassen`60): $T(n)=7T(n/2)+\Theta(n^2)$ (scheme with 7 recursive multiplications of $n/2\times n/2$ submatrices).
\item Quickselect (finding $i$'th smallest element): $T(n)=\max\{T(\vert \text{left part}\vert),\\ T(\vert\text{right part}\vert)\}+\Theta(n)$

\begin{algorithm}
    \caption{QuickSelect Algorithm}
    \begin{algorithmic}[1] % [1] means showing line numbers
    \State \textbf{Input:} Array $A$, indices $l$ and $r$, integer $i$
    \Function{QuickSelect}{$A, l, r, i$}
        \If{$l = r$}
            \State \Return $A[l]$
        \EndIf
        \State $q \gets \text{PARTITION}(A, l, r)$
        \State $k \gets q - l + 1$
        \If{$i = k$}
            \State \Return $A[q]$
        \ElsIf{$i < k$}
            \State \Return \Call{QuickSelect}{$A, l, q-1, i$}
        \Else
            \State \Return \Call{QuickSelect}{$A, q+1, r, i-k$}
        \EndIf
    \EndFunction
    \end{algorithmic}
\end{algorithm}
    \begin{itemize}
    \item Best Case: $\Theta(n)$
    \item Worst Case: $\Theta(n^2)$: This happens when we have sorted A. We may improve this to get $\Theta(n)$. How? First method is to use random pivot. 
    Second method, which is deterministic, would be to choose a pivot element judiciously. Group $n$ elements into $\lceil n/5 \rceil$ groups and calculate median for each group.  Then 
    we wil get $T(n)\geq T(\lceil n/5\rceil) + T(\frac {7n}{10}+2)+\Theta(n)$, which gives $T(n)=\Theta(n)$.
    \item Typical case: $\Theta(n)$
    \end{itemize}
\begin{algorithm}
    \caption{QuickSort Algorithm}
    \begin{algorithmic}[1]
    \State \textbf{Input:} Array \( A \), indices \( l \) and \( r \)
    \Function{QuickSort}{$A,l,r$}
        \If{$l<r$}
            \State $q \gets \text{PARTITION}(A, l, r)$
            \Call{QuickSort}{$A,l,q-1$}
            \Call{QuickSort}{$A,q+1,r$}
        \EndIf
    \EndFunction
    \end{algorithmic}
\end{algorithm}
\item Shell Sort: pseudocode is as follows.
\begin{verbatim}
Algorithm ShellSort(A: array of items, N: size of array)
Begin
void shellsort(int v[], int n){
    int gap, i, j, temp;

    for (gap = n/2; gap > 0; gap /= 2)
        for (i = gap; i < n; i++)
            for (j=i-gap; j>=0 && v[j]>v[j+gap]; j-=gap){
                temp = v[j];
                v[j] = v[j+gap];
                v[j+gap] = temp;
            }
}
End
\end{verbatim}
The idea is to do the insertion sort with a sequence of gaps, starting from something larger. For larger arrays, this can result in efficient algorithm.

It takes $\Theta(n^{1.25})$ in average.
\end{itemize}
\end{example}

\section{Dynamic Programming}
\begin{itemize}
\item This reduces the problem to smaller problems like D \& C
but possibly overlapping subproblems. For that reason, we need to memoize.
\end{itemize}
\section{Greedy Algorithm}

\section{Graph, BFS/DFS, Minimum spanning(Prim's, Kruskal's)}
\begin{itemize}
\item Three data structures to represent graph: list of edges, 
adjacency matrix, adjacency lists(could use doubly linkedlist, dictionary, etc). 
\item We could compare and see the tradeoff between 
space complexity, finding specific edge, and list 
of edges of particular node.
\item We can devise simple seach algorithms using simple set theory...
\item Some more sophisticated search strategies include BFS, DFS, MST, etc.
\begin{algorithm}[H]
\caption{BFS}
\begin{align*}
&\text{1. initialize all nodes and put }d[v]=\infty \\
&\text{2. while }Q\neq \emptyset \\
& \quad \text{v = }Dequeue(Q) \\
& \quad\text{ for each adjacent node of }v\in V, \\
& d[v]=d[u]+1, p[v] = u\text{, and }Enqueue(Q,v)
\end{align*}
\end{algorithm}
\begin{theorem}
$\forall v, d[v]=$length of shortest path from $s$ to $v$
\end{theorem}
\begin{proof}
Note that $d[u]=k$ or $k+1$ if $u\in Q$. Further, $d[u]\leq k$ if $u\in Done$ for some $k$.
We prove the $\leq$ direction since $\geq$ is clear. Induct on the length of
shortest path from s to v. Let's say this path goes through u. So we have $s-u-v$. By induction
hypothesis, we have $d[u]\leq \text{ length of shortest path from s to u}$.

Note now that $d[v] \leq d[u]+1$ after u is processed. Further, we have
$d[u]+1\leq\text{ shortest length from s to v}$ by our construction.
\end{proof}
\item The key of this proof is to identify the point where $u$ is just processed. This makes the proof
rather precise.
\item Partition into layers... in this case there will be no ``jump'' to the figure.
\item \textbf{DFS} can be used for finding the number of connected components, cyclicity of undirected graph(using parent information), etc
\item Can also discuss discovery time and finishing time and their properties (such as being only nested or disjoint).
\item We can talk about the structure of the tree: for instance for undirected graphs, there shouldn't be any cross edges.
\item We needed to implement stack/queue, and in python, we can just append an element to the array and use the pop method... the operation will take the usual O(1) due to dynamic array in python.
\item DFS can be used to find 1) DAG(directed acyclic graphs iff topological sort) and 2) SCC.
\item SCC: we do DFS on some s and then do DFS's again in the reverse order of finishing time. The key for the proof is to use
the tree structure of DFS.
\item If there isn't any back edge on DFS, that means the graph is acyclic
\end{itemize}
\subsection{Dijkstra's algorithm}
\begin{itemize}
\item The algorithm is based on $d(v) = \min\{d(u)+w(u,v)\mid \text{ all edges (u,v) }\}$
\end{itemize}
\bigsection{Miscelleneous}
\section*{Spectral clustering}
\begin{definition}
$Ncut(A,B)$ is defined as $$Ncut(A,B) = cut(A,B)(\frac 1{d(A)}+\frac 1{d(B)})$$ where $d(A)=\sum_{i\in A}d_i$.
\end{definition}
\begin{remark}
\begin{itemize}
\item The intuition is that when $A$ is relatively small, the $\frac 1{d(A)}$ will be large, hence discouraging the isolating small groups.
\item Finding minimum Ncut is equivalent to finding vector $v$ that minimizes $$\frac{v^T Lv}{v^tDv}\text{such that }v^tD1=0,v_i\in\{a,b\}$$ where 
$L=D-W$.
\item The thinking goes like this... $M(b,i)=\max\{M(b-w_i,i-1)+v_i, M(b,i-1)\}\text{ if }b\geq w_i
\text{ else }M(b,i)=M(b,i-1)$ where $M(b,i)$ is a maximum value
we can get with knapsack of capacity $b$, uwing a subset of the first
$i$ items only.
\end{itemize}
\end{remark}

\section*{Algorithms for Linearly Separable Data}
There are several ways. First, linear programming.
\begin{algorithm}[H]
        \caption{Linear Programming for Classifier}
        \begin{align*}
        \textbf{Objective:} \quad & \text{minimize} \quad \mathbf{u} = (0, \ldots, 0) \quad \text{(dummy variable)} \\
        \textbf{Subject to:} \quad & A\mathbf{w} \geq \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix} \\
        & A_{i,j} = y_i x_{i,j} \quad \text{(where \(j\)'th element of the vector \(x_i\))}
\end{align*}
\end{algorithm}
\begin{remark}
\begin{itemize}
\item $u$ is a dummy variable here; we essentially only check if the constraint is satisfied.
\item This is only applied for when the data is separable.
\item We can formula the regression problem with loss function $l(h, (x,y))=\lvert h(x)=y\rvert$ using linear programming.
\end{itemize}
\end{remark}

\begin{algorithm}[H]
    \caption{Batch Perceptron}
    \begin{algorithmic}[1]
    \Function{BatchPerceptron}{$x_1$, $y_1$, $\ldots$, $x_m$, $y_m$}
        \State $w(1) \gets (0, \ldots, 0)$
        \For{$t \gets 1, 2, \ldots$}
            \If{there exists $i$ such that $y_i \langle w(t), x_i \rangle \leq 0$}
                \State $w(t+1) \gets w(t) + y_i x_i$
            \Else
                \State \textbf{output} $w(t)$
                \State \textbf{break}
            \EndIf
        \EndFor
    \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{remark}
\begin{itemize}
\item Note that $y_i(w^{(t+1)}, x_i)=y_i(w^{(t)}, x_i)+\lVert x_i\rVert^2$
\item The algorithm must stop after at most $(RB)^2$ iterations, where $R=\max_i\lVert x_i\rVert$ represents a data spread, and \\
$B=\min\{\lVert w\rVert: i\in [m], y_i\langle w,x_i\rangle\geq 1\}$ represents 
margin.
\item To prove this, it suffices to show that $1\geq\frac{\langle w^*, w^{(T+1)}\rangle}{\lVert w^*\rVert\lVert w^{(T+1)}\rVert}\geq\frac{\sqrt T}{RB}$, which we proceed by bounding numerator and denominator separately.
\item We can prove that this bound is tight. For some vector $w^* \in \mathbb{R}^d$, the algorithm incurs $m = (BR)^2$ errors (considering $m = d$).
\item Moreover, for $d = 3$, an algorithm can be designed to commit exactly (m) errors for any given $m \in \mathbb{N}$, serving as an upper bound concurrently        \end{itemize}
\end{remark}

\section*{Logistic Regression}
\begin{definition}
Fit the logistic function $\phi_{sig}(x)=\frac 1{1+exp(-\langle w,x\rangle)}$
 with minimization scheme 
\[
w=argmin_{w\in\mathbb{R}^d} \frac 1m\sum^m_{i=1}
\log(1+exp(-y_i\langle w,x_i\rangle))
\]
\end{definition}
\begin{remark}
\begin{itemize}
\item The explaratory variable is between 0 and 1, making it interpretable as a probability.
\item Appropriate for binary classification.
\item Logistic loss function is a convex function so it's efficient to minimize.
\end{itemize}
\end{remark}

\section*{Variational Inference and EM algorithm}
\begin{definition}
Kullback-Leibler(KL) divergence between p.d.f.s g and f is given by
    $$d_{KL}(g\|f)=E_g[\log(\frac gf)]$$
\end{definition}
This is always nonnegative and it can be shown by Jensen's inequality. Intuitively, 
we have more 'confidence' on g whenever g is greater than f, whence the logarithm is positive.
\begin{definition}
The ELBO (Evidence Lower-Bound) of a p.d.f. g with respect to an unnormalized p.d.f. $\tilde{f}$ is given by

$$ELBO(g):=E_g[\log \frac{\tilde{f}}{g}]$$
\end{definition}
\begin{remark}
\begin{itemize}
\item Simple algebra yields $ELBO(g)\leq \log c$, where c is a normalizing constant.
\item Let's think in Bayesian framework; our unnormalized function in this case is
    $\tilde{f}(\theta):=f(y\vert \theta)f(\theta)$, so $ELBO(g)\leq\log f(y)$.
\item This justifies the name "evidence lower-bound" and this helps with the choice of modeling (essentially maximizing ELBO).
\item We write ELBO(g), but really, what's omitted is that this is with respect to $\tilde f(\theta)$.
\end{itemize}
\end{remark}
\begin{remark}
    \begin{itemize}
\item It follows that $ELBO(g)=E_g[\log f(y\rvert \theta)]-d_{KL}(g(\theta)\| f(\theta))$
\item In another Bayesian framework, similar equality (will be used in EM algorithm) is
\begin{align*}
        & ELBO(g,\theta)\\
        & =\int \log(\frac{f(y,z\vert\theta)}{g(z)})g(z)dz\\
        & =-d_{KL}(g(z)\| f(z\vert y,\theta))+\log(f(y\vert\theta))
\end{align*}
\end{itemize}
\end{remark}
The first term promotes matching the data, and the second term promotes matching prior beliefs.
The reason we work with log domain is that, except for obvious reasons, it helps with numerical stability.
\\
Indeed, remark 2 motivates what's called EM algorithm.
\begin{algorithm}
    \caption{EM Algorithm}
    \begin{algorithmic}[1] % [1] means showing line numbers
    \State \textbf{Input:} Initialization of $\theta_0$
    \Repeat
        \State \textbf{E-step:} compute 
        \[
        E_{Z\sim f(z\mid y,\theta_l)}[\log f(y,Z\mid \theta)] = \int \log f(y,z\mid \theta)f(z\mid y,\theta_l)dz
        \]
        \State \textbf{M-step:} compute
        \[
        \theta_{l+1} = \text{arg max}_\theta E_{Z\sim f(z\mid y,\theta_l)}[\log f(y,Z\mid \theta)]
        \]
    \Until{some stopping criterion}
    \end{algorithmic}
\end{algorithm}

\begin{theorem}
We have $\log f(y\vert \theta_l)\leq \log f(y\vert \theta{l+1})$
\end{theorem}
\begin{proof}
Let $g_l(z)=f(z\vert y,\theta_l)$. We have

\begin{align*}
    \log f(y\vert\theta_{l+1}) &= ELBO(g_l,\theta_{l+1})+d_{KL}(g_l| f(z\vert y,\theta_{l+1})) \\
    &\geq ELBO(g_l, \theta_l)+d_{KL}(g_l| f(z\vert y,\theta_l)) \\
    &= \log f(y\vert \theta_l)
    \end{align*}

\end{proof}
\begin{itemize}
\item This shows that likelihood function is non-decreasing for each iteration, and since likelihood is always bounded by 1, we have established the convergence
of the algorithm.
\item GMM algorithm is a specific instance of this algorithm. Here, $w_{ik}$ can be thought of as latent variable, 
corresponding to E-step, and computing $\theta$ and $\pi$ corresponds to M-step.
\item E-step can be computationally expensive and so we usually use Monte-Carlo method to approximate.
\end{itemize}
\begin{remark}
\begin{itemize}
\item Let us now venture into variational inference, which we use when approximating the intractable distribution. For the choice of possible sets that $f$ admits in $d_{KL}(g\|f)$, we can use \textbf{mean field family}, 
which assumes the independence for coordinate distributions. 
\end{itemize}
\end{remark}
\begin{theorem}
Let $g(x)=\prod^d_{i=1} g_i(x_i)$ with $g_{-i}(x_{-i})$ fixed. Then $$g_i^*(x_i)\propto \exp(E_{g_{-i}}[\log f(x_i, x_{-i}]))$$ maximizes
    $ELBO(g)$.
\end{theorem}
\begin{proof}
By routine algebra (we need to use independence at some point), one can show that $$ELBO(g)=E_{g_i}[\log (\exp(E_{g_{-i}}[\log f(x_i,x_{-i})]))-\log g_i(x_i)]+C$$
and notice that the first term can be phrased as -$d_{KL}(g^*\| g)$, whence $g^*=g$ gives the optimization solution. 
\end{proof}
\begin{remark}
\begin{itemize}
\item The intuition is to average out the effect of $x_{-i}$ on log expected value in order to incorporate the independence between $g_i$'s. 
\item This leads to the CAVI algorithm, which approximates the unnormalized target density $\tilde{f}$. After initialization, 
updating $g_i$ will increase ELBO for each $i$, so may iterate until ELBO converges.
\item The disadvantage is that it may be computationally expensive and accuracy might be not so good. 
\end{itemize}
\end{remark}

\section*{ Use FFT for circulant matrix}
\begin{algorithm}
    \caption{Fast Fourier Transform (FFT) for Circulant Matrix Multiplication}
    \begin{algorithmic}[1] % [1] means showing line numbers
    
    \State \textbf{input:} Circulant matrix $A \in \mathbb{R}^{n \times n}$, vector $x \in \mathbb{R}^n$
        
    \State Compute the FFT of the first column of $A$, denote it as $A_{FFT}$
    
    \State Compute the FFT of vector $x$, denote it as $x_{FFT}$
    
    \State Perform the Hadamard (element-wise) product of $A_{FFT}$ and $x_{FFT}$ to get the result vector $y_{FFT}$
    
    \State Compute the inverse FFT of $y_{FFT}$ to get the final result vector $y$
    
    \State \Return $y$
    
    \end{algorithmic}
    \end{algorithm}
    
\begin{itemize}
\item The time complexity is $O(n\log n)$ (versus the usual $O(n^2)$).
\item proof: 
\[
Cx=c_1 * x=IFFT(FFT(c_1)\circ FFT(x))
\]
where * is a circular convolution and $\circ$ is a hadamard product.
\begin{fact}
\[
\mathcal{F}\{x\cdot y\}
\]
is a circular convolution of $\mathcal{F}\{x\}$ and $\mathcal{F}\{y\}$.
\begin{example}
Set \( x = y = \begin{bmatrix}
    0 \\
    1 \\
    2 \\
    \end{bmatrix}.
     \)
The equality, at least for the first element, yields since
\[
3 + 2\exp\left(\frac{-2\pi i}{3}\right) + 2\exp\left(-\frac{-4\pi i}{3}\right) = 9 = 1 + 4 + 4
\]
\end{example}
\end{fact}
\begin{fact}
The inverse transform is given by
\[
x_n=\frac 1N \sum\limits^{N-1}_{k=0}X_k\cdot \exp(\frac{i2\pi}N kn)
\]
\end{fact}
\item This shows how the vector is decomposed with basis in frequency domain.
\item This is the analogous transform of Fourier 
Series (for a regular periodic function) and Fourier transform 
(for a regular non-periodic function).
\item The essence of the theorem is the interchange between multiplication and convolution.
\end{itemize}

\section*{Hashing}
\begin{itemize}
\item One example would be to add up all ASCII number,
 divide by the size of the list, and do the modulo (this whole process is a ``hash function") by the
  size of an array (usually a prime number).
\item Now, if we end up getting the same number, we can further use linkedlist to store different nodes.
\end{itemize}

\end{document}