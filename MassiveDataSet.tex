\documentclass[11pt,reqno]{amsart}
\usepackage{graphicx, url}
\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{titlesec}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\graphicspath{ {./images/} }
\usetikzlibrary{shapes}
\usepgfplotslibrary{polar}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{backgrounds}
\pgfplotsset{every axis/.append style={
                    axis x line=middle,    % put the x axis in the middle
                    axis y line=middle,    % put the y axis in the middle
                    axis line style={<->,color=blue}, % arrows on the axis
                    xlabel={$},          % default put x on x-axis
                    ylabel={$},          % default put y on y-axis
            }}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\usepackage[utf8]{inputenc}

\newtheorem{fact}{Fact}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{remark}[example]{Remark}

\newcommand{\bigsection}[1]{
\titleformat*{\section}{\centering\LARGE\bfseries}
\section*{#1}
\titleformat*{\section}{\large\bfseries} % Reset to the original format
}
\titleformat{\section}
{\normalfont\Large\bfseries\centering}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\large\bfseries\centering}{\thesubsection}{1em}{}


\title{Data Structure and Algorithm for Massive Dataset}
\author{MinSeok Song}
\date{}
\pgfplotsset{compat=1.18}
\begin{document}
\maketitle
\begin{sloppypar}
\tableofcontents
\section{Three ways to deal with massive dataset}
\begin{enumerate}
\item Dimensional reduction: the purpose is to minimize the loss of information.
\item Compressed representation: present data in a compact form, but not necessarily predicated on the retainment 
of information. i.e., it may prefer higher compression rates.
\item Interpolation: only use discrete information of the distribution $f$. 
This is useful since we do not have a full function $f$ available. Remember we 
used finite element method in numerical PDE, and the right space of function to discuss
 numerical stability etc was Sobolev space.
\end{enumerate}
\begin{itemize}
\item All in all, it focuses on achieving lower computational/statistical complexity.
\item To clarify, computational complexity deals with the resources(time and space), 
while statistical complexity with the intricacy of models(in the sense of how simpler model represents reduced data). 
\end{itemize}
\begin{theorem} (Johnson-Lindenstrauss Lemma)
Let $Q$ be a finite set of vectors in $\mathbb{R}^d$. Let $\delta\in (0,1)$ and $n$ be large enough integer such that
\begin{equation}
\epsilon = \sqrt{\frac{6\log (2\lvert Q\rvert/\delta)}n}\leq 3
\end{equation}
With probability of at least $1-\delta$ over a choice of a random matrix $W\in\mathbb{R}^{n,d}$
 such that each element of $W$ is distributed normally with zero mean and variance of $1/n$ we have
\begin{equation}
\sup_{x\in\mathbb{Q}}\lvert \frac{\lVert Wx\rVert^2}{\lVert x\rVert^2}-1\rvert < \epsilon
\end{equation}
\end{theorem}
\begin{itemize}
\item We might think that $W$ needs to be closer to identity matrix, but this lemma is all about preserving the distance.
\item Do note that each element of $W$ is generated by $N(0,1/n)$, that $n$ (the count of our vectors) is used here.
\item The proof leans on the following lemma, which uses the concentration property of $\chi^2$.
\end{itemize}
\begin{lemma}\label{lem:concentration}
Fix some $x\in\mathbb{R}^d$. Let $W\in\mathbb{R}^{n,d}$ be a random matrix such that each $W_{i,j}$
 is an independent normal random variable. Then, for every $\epsilon\in (0,3)$ we have
 \begin{equation}
 \mathbb{P}[\lvert \frac{\lVert (1/\sqrt n)Wx\rVert}{\lVert 
    x\rVert}-1 \rvert>\epsilon]\leq 2e^{-\epsilon^2 n/6}
 \end{equation}
\end{lemma}
\begin{itemize}
\item Note that $W:\mathbb{R}^d\to\mathbb{R}^n$, and the result does not depend on d. This suggests
 that we can conduct dimensionality reduction in very high-dimensional spaces without much cost(!).
\item This shows the existence of $T=\frac{1}{\sqrt k}\cdot R$ with
 $R\in \mathbb{R}^{k\times d}$, each element generated by $N(0,1)$, when 
 $k\geq \Omega(\frac {\log{k\lvert Q\rvert}/\delta}{\epsilon^2})$, where $k$ depends on $\delta$ as well. 
\end{itemize}
\begin{proof}[Proof of Lemma \ref{lem:concentration}]
We can assume, WLOG, that $\lVert x\rVert^2=1$. Do note that $\lVert Wx\rVert^2$ has a $\chi_n^2$ distribution
 by construction, so we may use concentration of $\chi^2$ inequality to get the result.
\end{proof}
\begin{proof}
In order to deal with $\lvert Q\rvert$, use the union bound. We can find appropriate $\epsilon$ afterward.
\end{proof}
\begin{itemize}
\item This says that the random projections do not distort Euclidean distances too much.
\end{itemize}

\section{Efficient PCA}
We aim at solving the problem 
\[
\arg\min_{W\in\mathbb{R}^{n,d}, U\in\mathbb{R}^{d,n}}\sum^m_{i=1}\lVert x_i-UWx_i\rVert^2_2
\]
\begin{itemize}
\item It is then shown that the optimal solution is caculated by computing the eigenvectors of $A=\sum^m_{i=1}x_i^Tx_i=XX^T$. 
This is the right eigenvectors of SVD. Do note that $x_i$ is each column of $X$.
\item This means the complexity is given by $O(d^3+md^2)$
\begin{enumerate}
\item $O(d^3)$ for computing the eigenvectors and eigenvalues of $A=XX^T$.
\item $O(d^2 m)$ for computing the covariance matrix $A$.
\end{enumerate}
\item Instead of using $XX^T$, we can use the eigenvector of $B=X^TX$, that is, 
$A(X^Tu)=\lambda(X^T u)$ where $u$ is an eigenvector of $B$.
\item This comes from the fact that $X X^TX u=\lambda X u$.
\item Do note that $B$ only requires calculating inner products $\langle x_i,x_j\rangle$.
\item This reduces our complexity to $O(m^3+dm^2)$, which is useful when d is very large.
\end{itemize}

\section{Perturbation theory}
\begin{itemize}
\item Instead of considering $Tx=X^*$, let's shift our focus on $X^* = X+\epsilon$.
\item $X^*$ is $r$-dimensional, and the QR decomposition gives $X^*=U^*z^*$, with 
$X^*$ being orthonormal.
\item Weyl's inequality relates to the singular values of perturbed matrix $X^*$.
\begin{theorem}[Weyl's theorem]\label{Theorem:Weyl}
Let $X^*=X+\epsilon$. Then
\[
\lVert \tilde\Lambda-\Lambda\rVert_2\leq\lVert\epsilon\rVert_2.
\]
where $\Lambda$ and $\tilde\Lambda$ are singular value matrices.
\end{theorem}
\item This says that eigenvalue is stable under perturbation. 
\begin{theorem}[Wedin's theorem]
Let $X=X^*+\epsilon$. Then 
\[
\lVert U_{(r,X)} U_{(r,X)}^T-U_{(r,X^*)} U_{(X^*)}^T\rVert_{F}\leq
\frac{\lVert\epsilon\rVert_{F}}{\sigma_r(X)-\sigma_{r+1}(X)}
\]
\end{theorem}
\item This theorem answers the question: if we slightly perturb our matrix, 
how much does the "important" subspaces (as captured by the dominant singular vectors)
 change? This change is inversely proportional to the gap at $r$'th singular value.
\item In light of QR decomposition, we have $X^* = U^* z^*$ for some 
$U^*$ with dimension $d\times r$ and $z^*$ with dimension $r\times N$. Let $X=X^*+\Delta$. By Wedin's inequality, we have
\[
\lVert U_r U_r^T \lvert x^{(i)}-x^{(j)}\rvert\rVert\leq (1+O(\frac{\lVert 
  \Delta\rVert_2}{\sigma_r(x^*)}))\lVert x^{(i)^*}
  -x^{(j)^*}\rVert_2+O(\lVert\Delta\rVert_2)
\]
\item Even This gives the bound of the perturbation of $x^{(i)}-x^{(j)}$ in terms 
of $\Delta$ and r'th singular value. 
\item SVD has computational cost $O(dN^2)$ and JL has computational cost 
$O(dkN)=O(\frac{N\log N}{\epsilon^2})$
\item We would still prefer the method in JL lemma.
\item If the data matrix has rank $r$-dimensional, we can only require $O(\frac{\log r}{\epsilon^2})$ (?).
 \item Going forward, in summary, we use the perturbation theory 
 + SVD to quantify the approximately dominant subspace of the matrix, and upgrade(?, wrong)
 JL lemma in reality using Weylâ€™s/Wedin's lemma .
\begin{proof}[Proof of Theorem~\ref{Theorem:Weyl}]
First, assume that the matrix is Hermitian. We have 
\[
\lambda_n(A)=\max_{\lVert x\rVert=1}x^TAx
\]
for symmetric matrix $A$.
 It follows that $\min\limits_{dim(A)n=i+1}\max\limits_{x\in A,
\lVert x\rVert=1}x^TAx$. Using this, we can prove that
\[
\lambda_i(A)+\lambda_j(B)\geq \lambda_{i+j-1}(A+B)
\] where each $\lambda$'s are ordered.
Similarly,
\[
\lambda_i(A)+\lambda_j(B)\leq \lambda_{i+j-n}(A+B)
\]
The first inequality shows that
\[
\lvert \lambda_i(A+B)-\lambda_i(A)\rvert\leq \lVert B\rVert_2
\]
which is equivalent to
\[
  \lvert \lambda_i(X+\epsilon)-\lambda_i(X)\rvert\leq \lVert \epsilon\rVert_2
\]
in the setup of our theorem.
We can generalize this by taking

\[
\begin{pmatrix}
  0 & M \\
  M^* & 0 \\
\end{pmatrix}
\]
which gives 
\[
\lvert \sigma_k(X+\epsilon)-\sigma_k(X)\rvert\leq\sigma_1(\epsilon)
\]
\end{proof}
\end{itemize}
\begin{fact}
If A is a bounded self-adjoint operator on a Hilbert space $\mathbb{H}$, then 
\[
\lVert A\rVert = \sup\limits_{\lVert x\rVert =1}\lvert\langle x, Ax\rangle\rvert
\]
\end{fact}
\begin{proof}
It suffices to show $\leq$. 
By definition, we have 
\[
\lVert A\rVert = \sup\limits_{\lVert x\rVert=1}\lVert Ax\rVert
=\sup\{\lvert\langle y, Ax\rangle\lvert\rvert\lVert x\rVert=1, \lVert
  y\rVert=1\}
\]
Now using polarization formula (choose appropriate $y$ to delete imaginary part), we get,
\[
\vert\langle y,Ax\rangle\lvert^2\leq \frac 14\alpha^2(\lVert x\rVert^2+\lVert y\rVert^2)^2
\]
\end{proof}
\begin{itemize}
\item The polarization identity relates the inner product to the norms of linear combination of vectors. This
 is given by
\[
\langle x, y\rangle = \frac 14(\lVert x+y\rVert^2-\lVert x-y\rVert^2+i\lVert x+iy\rVert^2-i\lVert x-iy\rVert^2)
=\sum\limits^3_{k=0}\lVert x+i^ky\rVert^2.
\]
\end{itemize}

\section{Randomized SVD}
\begin{itemize}
\item We are interested in finding an appropriate $L_1$ for $Y\approx L_1 L_1^T Y$.
\item Setup:
\begin{itemize}
\item m: original dimension
\item n: number of data
\item k: target dimension
\item r: random number. 
\item $A:m\times n,L_1:m\times k, L_2: n\times k, V_r:n\times r, G: n\times k, G_1:(n-r)\times k,G_2:r\times k$
\end{itemize}
\item We may proceed like this.
\begin{enumerate}
\item Multiply random matrix $G$ on the right side of $A$ ($O(mnk)$).
\item Perform QR decomposition, so that $AG=L_1 R$ ($O(k^2m)$).
\item Compute $L_2=L_1^T A$ and use $L_1 L_2A$ ($O(mnk)$).
\end{enumerate}
\item Note that we have $L_1L_1^TAG=L_1L_1^TL_1R=L_1R=AG$.
\item In what sense is $L_1 L_1^T A\approx A$?
\begin{enumerate}
\item We can use \href{https://www.math.uci.edu/~rvershyn/teaching/2006-07/280/lec10.pdf}{Gordon's inequality}
(in random matrix theory) to show that
\[
E(\lVert A-L_1L_1^T A\rVert)\leq [1+O(\frac{\sqrt{n}
+\sqrt{k}}{\sqrt{k}-\sqrt{r}})]\lVert \Sigma_{>r}\rVert
\]
\item We can do "better":
\[
E(\lVert A-L_1L_1^T A\rVert_F^2)\leq (1+O(\frac{\sqrt{r}}{\sqrt{k}
-\sqrt{r}})^2)\lVert\Sigma_{>r}\rVert_F^2
\]
\end{enumerate}
\item Analogous statement exists for spectral norm explained \href{chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/0909.4061.pdf}{here}. 

\item Let us take $A_k=L_1L_1^T\in\mathbb{R}^{m\times m}$.
Let us take $X^*=U^*z^*$ and $X=X^*+\Delta$.
By the above inequality, we have
\begin{align*}
\lVert U_r^*U_r^{*T}-U_rU_r^T\rVert_F & \leq\frac{\Delta_F+C(k,r)\lVert\Sigma_{>r}(X)\rVert_F}{\sigma_r(X^*)-\sigma_{r+1}(X_k)}\\
&\leq\frac{\Delta_F+C(k,r)\lVert\Sigma_{>r}(X)\rVert_F}{\sigma_r(X^*)-(\sigma_{r+1}(X^*)+\lVert \Delta\rVert_F+C(k,r)\lVert \Sigma_{>r}(X)\rVert_F)}\\
&\leq\frac{\Delta_F+C(k,r)\lVert\Sigma_{>r}(X)\rVert_F}{\sigma_r(X^*)-(\sigma_{r+1}(X^*)+\lVert \Delta\rVert_F+C(k,r)\lVert \Sigma_{>r}(X)\rVert_F)}
\end{align*}
where $U_r$ comes from $X$, $U^*$ comes from $X^*$.
\item The second inequality comes from Weyl, and the third inequality from the inequality we acquired above.
\item Since $n$ is large, this can be costly; some variants to do this faster.
\begin{enumerate}
\item Structured random matrix; apply \href{https://www.mathc.rwth-aachen.de/~rauhut/files/LinzRauhut.pdf}{DFR decomposition} on G, and we can calculate $AG$
 in better time complxity (this involves FFT)
\item Interpolation; use \href{https://docs.oracle.com/en/database/oracle/machine-learning/oml4sql/21/dmcon/cur-matrix-decomposition.html#GUID-9C3BF153-3A2B-4B4B-B711-D48976806799}
{CUR decomposition}.
\item Verify if the matrix has certain structures (like Toeplitz).
\item Process by blocks.
\item Adaptive methods; start with a small rank and increase it adaptively.
\item Perform SVD on a smaller matrix.
\item Power iterations algorithm.
\end{enumerate}
\end{itemize}

\section{Kernel trick}
\begin{itemize}
\item Imagine
\[
\min\limits_w f(\langle w,\psi(x_1)\rangle,\dots,\langle w,\psi(x_m)\rangle)+R(\lVert w\rVert)
\]
where $f:\mathbb{R}^m\to \mathbb{R}$ is an arbitrary function, $R:\mathbb{R}_+\to\mathbb{R}$ is a monotonically nondecreasing function, and 
$\psi$ is a mapping from $\mathcal{X}$ to a Hilbert space.
\begin{theorem}(Representer Theorem)
$w=\sum^m_{i=1}\alpha_i\psi(x_i)$ for some $\alpha\in\mathbb{R}^m$ gives an optimal solution.
\end{theorem}
\item Note that $\psi$ is an intereseted embedding function to the higher dimensional space.
\begin{proof} 
Use the fact that Hilbert space has a basis (due to Gram-Schmidt), and express $w^*=\sum\limits^m_{i=1}\alpha_i\psi(x_i)+u$ where $w^*$ is an optimal solution. 
We have, by construction, 
\[
\lVert\sum\limits^m_{i=1}\alpha_i\psi(x_i)\rVert\leq \lVert w^*\rVert
\]
On the other hand, we have 
\[
f(\langle w,\psi(x_1)\rangle,\dots,\langle w,\psi(x_m)\rangle)=f(\langle w^*,\psi(x_1)\rangle,\dots,\langle w^*,\psi(x_m)\rangle)
\]
Combining these, it follows that $w$ gives an optimal solution.
\end{proof}
\item Now substituting this formula, we have 
\[
\min\limits_{\alpha\in\mathbb{R}^m}f(\sum\limits^m_{j=1}\alpha_jK(x_j,x_1),\dots,\sum\limits^m_
{j=1}\alpha_jK(x_j,x_m))+R(\sqrt{\sum\limits^m_{i,j=1}\alpha_i\alpha_jK(x_j,x_i)})
\]
where $K(x,x')=\langle \psi(x),\psi(x')\rangle$
\item Note that we now only need to calculate the value of $K$.
\begin{definition}
$G_{i,j}=K(x_i,x_j)$ is called Gram matrix.
\end{definition}
\begin{example}
Consider $k$ degree polynomial kernel defined to be $K(x,x')=(1+\langle x,x'\rangle)^k$. Then we have
\[
K(x,x')=\sum\limits_{J\in\{0,1,\dots n\}^k} \prod\limits^k_{i=1}\ x_{J_i}\prod\limits^k_{i=1}x'_{J_i}
=\langle \psi(x),\psi(x')\rangle
\]
by putting 
\[
\psi(x)=[\prod x_{J_{1,i}},\prod x_{J_{2,i}},\dots, \prod x_{J_{(n+1)^k,i}}]
\]
\end{example}
\begin{example}
Let $K(x,x')=\exp(-\frac{\lVert x-x'\rVert^2}{2\sigma})$.
By setting $\psi(x)=\frac{1}{\sqrt{n!}}\exp(-\frac{x^2}{2})x^n, n=0,1,\dots$, we have
\[
\langle \psi(x),\psi(x')\rangle=\exp(-\frac{\lVert x-x'\rVert^2}{2})
\]
\end{example}
\item In gerneral, symmetric $K$ is Kernel if and only if the associated gram matrix is positive semidefinite.
\item For a given Kernel, we usually perform eigenvalue decomposition, which leaves us with infinite dimensional space, 
which is not suitable computationally.
\item As a precursor, in order to decopose Kernel, we use the philosophy of interpolative decomposition (which doesn't work quite well for sparse matrix)
 and the detail of the kernel approximation is explained \href{https://arxiv.org/pdf/2006.02545.pdf}{here}.
\item Polynomial approximation to kernel can be found \href{https://arxiv.org/pdf/1109.4603.pdf}{here}.
\end{itemize}

\subsection*{Kernel approximation}
\begin{itemize}
\item We want to approximate a kernel $K:\Omega\times\Omega\to\mathbb{R}$. Let $[b_1,b_2,\dots,b_m]$ be polynomial basis, with each $b_i\in\mathbb{R}^{\infty\times m}$.
\item To do that: say $\lVert K(x,y)-\sum\limits^m_{i,j=1}b_i(x)\tilde\alpha_{ij}b_j(y)\rVert_{L^2(\Omega\times\Omega)}\leq\epsilon^m_A$, an approximation error. Try to find $\tilde\alpha$ such that
\[
K\approx B\tilde \alpha B^T
\]
\item Suppose $B$ satisfies $\langle b_i,b_j\rangle=\delta_{ij}$. 
\item We can choose $\hat\alpha$ by truncating $B$ to finite dimension. This procedure involves integration error. That is, 
\[
\tilde\alpha=\langle b_i, (Kb_j)\rangle\approx\sum_{l=0}^{m}b_i(q_l)(Kb_j)(q_l)w_l+\epsilon_I=\hat\alpha_{ij}+\epsilon_I
\]
\item It follows that 
\[
\lVert K-B\hat \alpha B^T\rVert_{L^2}\leq \epsilon_A+\epsilon_I
\]

Let $K=K_m+E$. We have
\begin{align*}
\hat\alpha &= \sum\limits^m_{l=0}b_i(q_l)(Kb_j)(q_l)w_l\\
&=\sum\limits^m_{l=0}b_i(q_l)(K_mb_j)(q_l)w_l+b_i(q_l)(Eb_j)(q_l)w_l
\end{align*}
Note that
\[
\sum\limits^m_{ij=1}b_i(q_l)(Eb_j)(q_l)w_l
\]
is bounded by approximation error scaled by m since $E=K-K_m$.
\item All in all, what we do is to 1) find a basis B (approximation error) and 2) pick a 
weight corresponding to nodes (integration error).
\item In the context of linear algebra, what we did is to 
\begin{enumerate}
\item First approximate by picking B: $B\tilde \alpha B^T=K$
\item approximate the corresponding $\alpha$ using quadrature and 
weight: $\bar B\hat \alpha\bar B^T=\bar K$ where 
$\bar B=[\sum_l b_{i,j}(q_l)]$ and $\bar K=[K(q_i,q_j)]$
\end{enumerate}
\item We can approximate Gaussian kernel by polynomials and get an error $m\sim O(1/\epsilon)$. This seems to involve the smoothness of the kernel though. 
In fact, we can impose less regularity on Kernel, which motivates CUR. Notice also the form $\bar B\hat \alpha\bar B^T$, which reminds us of CUR decomposition.
\end{itemize}

\section{Connection to CUR}
\begin{itemize}
\item Let's deviate a bit. In CUR decomposition, what happens if C and R are degenerate? The following existential theorem gives the answer for the worst case.
\begin{theorem}[\href{ttps://www.sciencedirect.com/science/article/pii/S0024379596003011}{ref}]
Let $A=A_r+F$ where $rank(A_r)\leq r$ and
$\lVert F\rVert_2\leq \epsilon=\sigma_{r+1}(A)$.
Then there exists $r$ column rows index I, J, and coefficient matrix G with dimension $r\times r$ such that
\[
\lVert A-CGR\rVert_2\leq \epsilon(1+\sqrt{\lVert \hat U^{-1}\rVert_2}+
\sqrt{\lVert \hat V^{-1}\rVert_2})
\]
where $A_r=U\Sigma V^T$, $\hat U=U(I, \cdot)$, and $\hat V=V^T(J,\cdot)$.
\end{theorem}
\begin{proof}
\begin{align*}
  A_r-CGR&=(U U^T + U_{\perp} (U_{\perp})^T)(A_r-CGR)(V V^T + V_{\perp} (V_{\perp})^T)\\
  &= 
\end{align*}
\end{proof}
\item The following proposition gives a further refinement of this bound.
\begin{proposition}
Let $I$ be chosen such that $|det(U(I))|$ is maximized for $U\in\mathbb{R}^{m\times r}$. Then
\[
\frac 1{\sigma_{\min}(U(I,\cdot))}\leq\sqrt{r(m-r)+1}
\]
\end{proposition}
\begin{proof}

\end{proof}
\item Going back to what we needed for performing CUR decomposition, we require,
\begin{enumerate}
\item find top singular vectors $U,V$ (in an attempt to get a better error bound).
\item find $I,J$ via maximum volume on $U,V$ (in an attempt to minimize the inverse of singular value). 
\end{enumerate}
\item Even with the cost, CUR has interpretability advantage since we're using the column verbatim.
\item With RSVD, the first procedure still takes $O(n^2)$; Second procedure is also combinatorially hard.
\item We can bypass the second one by using greedy algorithm ("good" locally). 
\item We want to avoid computing $U$ and $V$. Can we do that?
\begin{theorem}
Consider 2 by 2 block matrix $\{A_{ij}\}$. Suppose that $A_{11}$ has maximal value over all
r by r principal submatrix of A. Then 
\[
\lVert A_{22}-A_{21}A_{11}^{-1}A_{12}\rVert_{\infty}\leq (r+1)\sigma_{r+1}(A)
\]
\end{theorem}
\begin{proof} (sketch)
Let us prove in the case where $A_{21}, A_{22}$ and $A_{12}$ are one by one.
We have $A^{-1}=\frac 1{det(A)}adj(A)$. Define $(r+1)\times(r+1)$ matrix
$A_2 = 
\begin{pmatrix}
A_{11} & a_{12}\\
a_{21} & a_{22}
\end{pmatrix}
$. Since $A_{11}$ has maximum determinant, maximum value is achieved in $(r+1,r+1)$ index. It follows that
\[
\sigma_{r+1}(A)^{-1}=\lVert A^{-1}\rVert_2\leq (r+1)|A^{-1}_{r+1,r+1}|=(r+1)|a_{22}-a_{21}A^{-1}_{11}a_{12}|^{-1}
\]
Rearranging the term, we get the result.
\end{proof}
\item Finding the maximum submatrix is NP hard problem, so let us address this by usign greedy algorithm.
\end{itemize}

\section{Greedy Adaptive Cross Approximation with complete pivot}
\begin{enumerate}
\item \textbf{Input:} Initialization of matrix $A$.
\item \textbf{Output:} Sets $I$ and $J$.
\item \textbf{Initialize:} $I = J = \emptyset$, $R_0 = A$.
\item \textbf{Iterate:} 
\begin{enumerate}
\item \textbf{Selection step:} Compute 
\[
(i_k, j_k) = \arg\max_{i \not\in I, j \not\in J} |R_{k-1}(i, j)|
\]
\item \textbf{Update step:} Update the sets $I$ and $J$:
\[
I = I \cup \{i_k\}, \quad J = J \cup \{j_k\}
\]
\item \textbf{Pivot step:} Set the pivot value $p_k$:
\[
p_k = R_{k-1}(i_k, j_k)
\]
\item \textbf{Matrix update:} Update the matrix $R_k$:
\[
R_k = R_{k-1} - \frac{1}{p_k}R_{k-1}(\cdot, j_k)R_{k-1}(i_k, \cdot)
\]
\end{enumerate}
\item \textbf{Repeat:} until some stopping criterion is met.
\end{enumerate}
\begin{itemize}
\item Heuristically, the last step is like subtracting rank-1 approximation.
\end{itemize}

\section{Intuition and Analysis of GACA}
\textbf{(i) intuition}
\begin{itemize}
\item This corresponds to LU factorization with pivoting.
\item Related to CUR decomposition, let us assume that $A=A(\cdot, J)A(I,J)^{+}A(I,\cdot)+
\begin{pmatrix}
0 & 0\\
0 & A^{(k)}
\end{pmatrix}$.
Let  
$A=\begin{pmatrix}
A_{11} & A_{12}\\
A_{21} & A_{22}
\end{pmatrix}$ and $A_{11}=L_{11}U_{11}$. By simply 
rewriting the above formula, it follows that 
\[A=
\begin{pmatrix}
L_{11} & 0\\
L_{21} & I
\end{pmatrix}
\begin{pmatrix}
I & 0\\
0 & A^{(k)}
\end{pmatrix}
\begin{pmatrix}
U_{11} & U_{12}\\
0 & I
\end{pmatrix}
\]
where $A^{(k)}=A_{22}-A_{21}A_{11}^{-1}A_{12}, L_{21}=A_{21}U_{11}^{-1}$ and $U_{12}=L_{11}^{-1}A_{12}$
\item Therefore, if we stop at step k, we can pull out LU factorization of it. 
\item The crucial part is that we do not need to repeat LU factorization for $A_{11}$ for each iteration.
\item We can use the above equality to show that finding $(i_k,j_k)$ in the greedy procedure
 is same as picking $\arg\max\limits_{i\not\in I,j\not\in J}\det(A(I\cup i, J\cup j))$; just note that 
\[
det(A(\tilde I, \tilde J))=det(L_{11})det(U_{11})\lvert A^{(k)}(i,j)\rvert
\]
\item This does not guarantee the global maximization of the volume of the submatrix, but we're picking the best possible index
 in each iteration(adding one more column and row).
\end{itemize}
\textbf{(ii) analysis}
\begin{definition}
$\rho_r = \sup_{\text{rank(A)$>$r}}\frac
{\lVert A^{(r)}\rVert_\infty}{\lVert A\rVert_\infty}$ is called growth factor.
\end{definition}
\begin{theorem}
Let $A\in\mathbb{R}^{n\times n}$ be at least rank $r$. Then 
\[
\lVert A-A(\cdot, J)A(I,J)^{-1}A(I,\cdot)\rVert_{\infty}\leq 4^r \rho_r\sigma_{r+1}(A)  
\]
\end{theorem}
\begin{itemize}
\item Usually, $\sigma_{r+1}(A)$ decreases exponentially.
\begin{proof}
Let $I=\{1,\dots,r\}$ and $J=\{1,\dots,r\}$. Let $A_{11}=A[\tilde I, \tilde J]$.
As before, we have 
\[A=
\begin{pmatrix}
\tilde L_{11} & 0\\
\tilde L_{21} & I
\end{pmatrix}
\begin{pmatrix}
I & 0\\
0 & A^{(r+1)}
\end{pmatrix}
\begin{pmatrix}
\tilde U_{11} & \tilde U_{12}\\
0 & I
\end{pmatrix}
\]
The key step involves using lemma related to the above equality, and use induction to get the diagonal elements of $L_{11}$ and $U_{11}$ (and their characteristics, namely that the diagonal elements of $L_{11}$ is maximum in its column).
We then use the lemma by Higham which gives the crucial bound, $\lVert A_{11}^{-1}\rVert\leq 4^r\min\{\lvert p_1\rvert,\dots,\lvert p_{r+1}\rvert\}^{-1}$. The rest is to use the definition of growth factor.
\end{proof}

\item Due to maximization step, we generally get $O(rn^2)$ complexity.
\item With additional assumption for positive semi definite matrix, we have $O(rn)$.
\end{itemize}
\section{Compressed Sensing}
\begin{itemize}
\item This method juxtaposes PCA method.
\begin{definition}
A matrix $W\in \mathbb{R}^{n,d}$ is $(\epsilon, s)-RIP$ (with $\epsilon<1$) if for all $x\neq 0$ s.t. $\lVert x\rVert_0\leq s$
 we have 
 \[
|\frac{\lVert Wx\rVert^2_2}{\lVert x\rVert^2_2}-1|\leq \epsilon
 \]
\end{definition}
\begin{theorem}
If $W\in\mathbb{R}^{n,d}$ is an $(\epsilon, 2s)\sim RIP$ matrix, 
and if $x$ has less than $s$ many nonzero elements, then $x$ is the unique sparsest vector that
 gets mapped to $Wx$ by the matrix $W$.
\end{theorem}
\begin{proof}
Assume $\tilde x\neq x$. Observe that 
$\lVert x-\tilde x\rVert\leq 2s$ and $W(x-\tilde x)=0$. 
On the other hand, $\lvert 0-1\rvert\leq \epsilon$, contradicting 
the definition of RIP matrix.
\end{proof}
\item This implies that, for specific compression matrices and sparse data, the original data 
can be accurately recovered.
\begin{theorem}
Let $\epsilon<\frac 1{1+\sqrt{2}}$ and let $W$ be a ($\epsilon, 2s$)-RIP matrix. Let
 $x$ be an arbitrary and let $x_s$ be the vector which equals $x$ for the $s$ largest
  elements of $x$ and equals 0 elsewhere. Let $x^*\in \arg\min\limits_{v:Wv=y}\lVert
    v\rVert_1$. Then
\[
\lVert x^*-x\rVert_2\leq 2\frac{1+\rho}{1-\rho}s^{-1/2}\lVert x-x_s\rVert_1
\]
where $\rho=\sqrt{2}\epsilon/(1-\epsilon)$.
\end{theorem}
\begin{remark}
In particular, we have 
\[
x=\arg\min\limits_{v:Wv=y}\lVert
v\rVert_0=\arg\min\limits_{v:Wv=y}\lVert v\rVert_1
\]
for $\lVert x\rVert_0\leq s$.
\end{remark}
\item We used $L^1$ since we are looking for
solutions that are "almost sparse" rather than strictly sparse. Further, 
$L^1$ is convex so we can compute efficiently.
\begin{theorem}
Let $\epsilon, \delta\in(0,1)$.
Let $U$ be orthonormal matrix of size $d\times d$. Further, 
let $W\in\mathbb{R}^{n,d}$ be generated by $N(0,1/n)$ where 
$n\geq 100\frac{s\log(40d/(\delta\epsilon))}{\epsilon^2}$ and $s\in [d]$. Then the matrix
$WU$ is $(\epsilon,s)$-RIP.
\end{theorem}
\begin{remark}
This is useful when the sparsity is hidden, i.e. $y=U\alpha$ where $y$ is sparse.
\end{remark}
\item Connection of PCA with compressed sensing: While PCA identifies the dominant 
subspace in which most of the data's energy or variance lies, compressed sensing 
exploits the fact that signals often have sparse representations in some domain. 
These two concepts are related in the sense that they both exploit inherent structures
in data (low-rank structure and sparsity, respectively) for efficient processing 
or recovery.
\item As another note, we can also exploit the rank of the data matrix.
\end{itemize}
\end{sloppypar}
\end{document}