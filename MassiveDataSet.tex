\documentclass[11pt,reqno]{amsart}
\usepackage{graphicx, url}
\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{titlesec}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\graphicspath{ {./images/} }
\usetikzlibrary{shapes}
\usepgfplotslibrary{polar}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{backgrounds}
\pgfplotsset{every axis/.append style={
                    axis x line=middle,    % put the x axis in the middle
                    axis y line=middle,    % put the y axis in the middle
                    axis line style={<->,color=blue}, % arrows on the axis
                    xlabel={$},          % default put x on x-axis
                    ylabel={$},          % default put y on y-axis
            }}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\usepackage[utf8]{inputenc}

\newtheorem{fact}{Fact}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{remark}[example]{Remark}

\newcommand{\bigsection}[1]{
\titleformat*{\section}{\centering\LARGE\bfseries}
\section*{#1}
\titleformat*{\section}{\large\bfseries} % Reset to the original format
}
\titleformat{\section}
{\normalfont\Large\bfseries\centering}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\large\bfseries\centering}{\thesubsection}{1em}{}


\title{Data Structure and Algorithm for Massive Dataset}
\author{MinSeok Song}
\date{}
\pgfplotsset{compat=1.18}
\begin{document}
\maketitle
\begin{sloppypar}
\tableofcontents
\section{Three ways to deal with massive dataset}
\begin{enumerate}
\item Dimensional reduction: the purpose is to minimize the loss of information.
\item Compressed representation: present data in a compact form, but not necessarily predicated on the retainment 
of information. i.e., it may prefer higher compression rates.
\item Interpolation: only use discrete information of the distribution $f$. 
This is useful since we do not have a full function $f$ available. Remember we 
used finite element method in numerical PDE, and the right space of function to discuss
 numerical stability etc was Sobolev space.
\end{enumerate}
\begin{itemize}
\item All in all, it focuses on achieving lower computational/statistical complexity.
\item To clarify, computational complexity deals with the resources(time and space), 
while statistical complexity with the intricacy of models(in the sense of how simpler model represents reduced data). 
\end{itemize}
\begin{theorem} (Johnson-Lindenstrauss Lemma)
Let $Q$ be a finite set of vectors in $\mathbb{R}^d$. Let $\delta\in (0,1)$ and $n$ be large enough integer such that
\begin{equation}
\epsilon = \sqrt{\frac{6\log (2\lvert Q\rvert/\delta)}n}\leq 3
\end{equation}
With probability of at least $1-\delta$ over a choice of a random matrix $W\in\mathbb{R}^{n,d}$
 such that each element of $W$ is distributed normally with zero mean and variance of $1/n$ we have
\begin{equation}
\sup_{x\in\mathbb{Q}}\lvert \frac{\lVert Wx\rVert^2}{\lVert x\rVert^2}-1\rvert < \epsilon
\end{equation}
\end{theorem}
\begin{itemize}
\item We might think that $W$ needs to be closer to identity matrix, but this lemma is all about preserving the distance.
\item Do note that each element of $W$ is generated by $N(0,1/n)$, that $n$ (the count of our vectors) is used here.
\item The proof leans on the following lemma, which uses the concentration property of $\chi^2$.
\end{itemize}
\begin{lemma}\label{lem:concentration}
Fix some $x\in\mathbb{R}^d$. Let $W\in\mathbb{R}^{n,d}$ be a random matrix such that each $W_{i,j}$
 is an independent normal random variable. Then, for every $\epsilon\in (0,3)$ we have
 \begin{equation}
 \mathbb{P}[\lvert \frac{\lVert (1/\sqrt n)Wx\rVert}{\lVert 
    x\rVert}-1 \rvert>\epsilon]\leq 2e^{-\epsilon^2 n/6}
 \end{equation}
\end{lemma}
\begin{itemize}
\item Note that $W:\mathbb{R}^d\to\mathbb{R}^n$, and the result does not depend on d. This suggests
 that we can conduct dimensionality reduction in very high-dimensional spaces without much cost(!).
\item This shows the existence of $T=\frac{1}{\sqrt k}\cdot R$ with
 $R\in \mathbb{R}^{k\times d}$, each element generated by $N(0,1)$, when 
 $k\geq \Omega(\frac {\log{k\lvert Q\rvert}/\delta}{\epsilon^2})$, where $k$ depends on $\delta$ as well. 
\end{itemize}
\begin{proof}[Proof of Lemma \ref{lem:concentration}]
We can assume, WLOG, that $\lVert x\rVert^2=1$. Do note that $\lVert Wx\rVert^2$ has a $\chi_n^2$ distribution
 by construction, so we may use concentration of $\chi^2$ inequality to get the result.
\end{proof}
\begin{proof}
In order to deal with $\lvert Q\rvert$, use the union bound. We can find appropriate $\epsilon$ afterward.
\end{proof}
\begin{itemize}
\item This says that the random projections do not distort Euclidean distances too much.
\end{itemize}

\section{Efficient PCA}
We aim at solving the problem 
\[
\arg\min_{W\in\mathbb{R}^{n,d}, U\in\mathbb{R}^{d,n}}\sum^m_{i=1}\lVert x_i-UWx_i\rVert^2_2
\]
\begin{itemize}
\item It is then shown that the optimal solution is caculated by computing the eigenvectors of $A=\sum^m_{i=1}x_i^Tx_i=XX^T$. 
This is the right eigenvectors of SVD. Do note that $x_i$ is each column of $X$.
\item This means the complexity is given by $O(d^3+md^2)$
\begin{enumerate}
\item $O(d^3)$ for computing the eigenvectors and eigenvalues of $A=XX^T$.
\item $O(d^2 m)$ for computing the covariance matrix $A$.
\end{enumerate}
\item Instead of using $XX^T$, we can use the eigenvector of $B=X^TX$, that is, 
$A(X^Tu)=\lambda(X^T u)$ where $u$ is an eigenvector of $B$.
\item This comes from the fact that $X X^TX u=\lambda X u$.
\item Do note that $B$ only requires calculating inner products $\langle x_i,x_j\rangle$.
\item This reduces our complexity to $O(m^3+dm^2)$, which is useful when d is very large.
\end{itemize}

\section{Perturbation theory}
\begin{itemize}
\item Instead of considering $Tx=X^*$, let's shift our focus on $X^* = X+\epsilon$.
\item $X^*$ is $r$-dimensional, and the QR decomposition gives $X^*=U^*z^*$, with 
$X^*$ being orthonormal.
\item Weyl's inequality relates to the singular values of perturbed matrix $X^*$.
\begin{theorem}[Weyl's theorem]\label{Theorem:Weyl}
Let $X^*=X+\epsilon$. Then
\[
\lVert \tilde\Lambda-\Lambda\rVert_2\leq\lVert\epsilon\rVert_2.
\]
where $\Lambda$ and $\tilde\Lambda$ are singular value matrices.
\end{theorem}
\item This says that eigenvalue is stable under perturbation. 
\begin{theorem}[Wedin's theorem]
Let $X=X^*+\epsilon$. Then 
\[
\lVert U_{(r,X)} U_{(r,X)}^T-U_{(r,X^*)} U_{(X^*)}^T\rVert_{F}\leq
\frac{\lVert\epsilon\rVert_{F}}{\sigma_r(X)-\sigma_{r+1}(X)}
\]
\end{theorem}
\item This theorem answers the question: if we slightly perturb our matrix, 
how much does the "important" subspaces (as captured by the dominant singular vectors)
 change? This change is inversely proportional to the gap at $r$'th singular value.
\item If we have a big gap, the subspace is so important that the small perturbation doesn't change much.
\item In light of QR decomposition, we have $X^* = U^* z^*$ for some 
$U^*$ with dimension $d\times r$ and $z^*$ with dimension $r\times N$. Let $X=X^*+\Delta$. By Wedin's inequality, we have
\[
\lVert U_r U_r^T \lvert x^{(i)}-x^{(j)}\rvert\rVert\leq (1+O(\frac{\lVert 
  \Delta\rVert_2}{\sigma_r(x^*)}))\lVert x^{(i)^*}
  -x^{(j)^*}\rVert_2+O(\lVert\Delta\rVert_2)
\]
\item Even This gives the bound of the perturbation of $x^{(i)}-x^{(j)}$ in terms 
of $\Delta$ and r'th singular value. 
\item SVD has computational cost $O(dN^2)$ and JL has computational cost 
$O(dkN)=O(\frac{N\log N}{\epsilon^2})$
\item We would still prefer the method in JL lemma.
\item If the data matrix has rank $r$-dimensional, we can only require $O(\frac{\log r}{\epsilon^2})$ (?).
 \item Going forward, in summary, we use the perturbation theory 
 + SVD to quantify the approximately dominant subspace of the matrix, and upgrade(?, wrong)
 JL lemma in reality using Weylâ€™s/Wedin's lemma .
\begin{proof}[Proof of Theorem~\ref{Theorem:Weyl}]
First, assume that the matrix is Hermitian. We have 
\[
\lambda_n(A)=\max_{\lVert x\rVert=1}x^TAx
\]
for symmetric matrix $A$.
 It follows that $\min\limits_{dim(A)n=i+1}\max\limits_{x\in A,
\lVert x\rVert=1}x^TAx$. Using this, we can prove that
\[
\lambda_i(A)+\lambda_j(B)\geq \lambda_{i+j-1}(A+B)
\] where each $\lambda$'s are ordered.
Similarly,
\[
\lambda_i(A)+\lambda_j(B)\leq \lambda_{i+j-n}(A+B)
\]
The first inequality shows that
\[
\lvert \lambda_i(A+B)-\lambda_i(A)\rvert\leq \lVert B\rVert_2
\]
which is equivalent to
\[
  \lvert \lambda_i(X+\epsilon)-\lambda_i(X)\rvert\leq \lVert \epsilon\rVert_2
\]
in the setup of our theorem.
We can generalize this by taking

\[
\begin{pmatrix}
  0 & M \\
  M^* & 0 \\
\end{pmatrix}
\]
which gives 
\[
\lvert \sigma_k(X+\epsilon)-\sigma_k(X)\rvert\leq\sigma_1(\epsilon)
\]
\end{proof}
\end{itemize}
\begin{fact}
If A is a bounded self-adjoint operator on a Hilbert space $\mathbb{H}$, then 
\[
\lVert A\rVert = \sup\limits_{\lVert x\rVert =1}\lvert\langle x, Ax\rangle\rvert
\]
\end{fact}
\begin{proof}
It suffices to show $\leq$. 
By definition, we have 
\[
\lVert A\rVert = \sup\limits_{\lVert x\rVert=1}\lVert Ax\rVert
=\sup\{\lvert\langle y, Ax\rangle\lvert\rvert\lVert x\rVert=1, \lVert
  y\rVert=1\}
\]
Now using polarization formula (choose appropriate $y$ to delete imaginary part), we get,
\[
\vert\langle y,Ax\rangle\lvert^2\leq \frac 14\alpha^2(\lVert x\rVert^2+\lVert y\rVert^2)^2
\]
\end{proof}
\begin{itemize}
\item The polarization identity relates the inner product to the norms of linear combination of vectors. This
 is given by
\[
\langle x, y\rangle = \frac 14(\lVert x+y\rVert^2-\lVert x-y\rVert^2+i\lVert x+iy\rVert^2-i\lVert x-iy\rVert^2)
=\sum\limits^3_{k=0}\lVert x+i^ky\rVert^2.
\]
\end{itemize}

\section{Randomized SVD}
\begin{itemize}
\item We are interested in finding an appropriate $L_1$ for $Y\approx L_1 L_1^T Y$.
\item Setup:
\begin{itemize}
\item m: original dimension
\item n: number of data
\item k: target dimension
\item r: random number. 
\item $A:m\times n,L_1:m\times k, L_2: n\times k, V_r:n\times r, G: n\times k, G_1:(n-r)\times k,G_2:r\times k$
\end{itemize}
\item We may proceed like this.
\begin{enumerate}
\item Multiply random matrix $G$ on the right side of $A$ ($O(mnk)$).
\item Perform QR decomposition, so that $AG=L_1 R$ ($O(k^2m)$).
\item Compute $L_2=L_1^T A$ and use $L_1 L_2A$ ($O(mnk)$).
\end{enumerate}
\item Note that we have $L_1L_1^TAG=L_1L_1^TL_1R=L_1R=AG$.
\item In what sense is $L_1 L_1^T A\approx A$?
\begin{enumerate}
\item We can use \href{https://www.math.uci.edu/~rvershyn/teaching/2006-07/280/lec10.pdf}{Gordon's inequality}
(in random matrix theory) to show that
\[
E(\lVert A-L_1L_1^T A\rVert)\leq [1+O(\frac{\sqrt{n}
+\sqrt{k}}{\sqrt{k}-\sqrt{r}})]\lVert \Sigma_{>r}\rVert
\]
\item We can do "better":
\[
E(\lVert A-L_1L_1^T A\rVert_F^2)\leq (1+O(\frac{\sqrt{r}}{\sqrt{k}
-\sqrt{r}})^2)\lVert\Sigma_{>r}\rVert_F^2
\]
\end{enumerate}
\item Analogous statement exists for spectral norm explained \href{chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/0909.4061.pdf}{here}. 

\item Let us take $A_k=L_1L_1^T\in\mathbb{R}^{m\times m}$.
Let us take $X^*=U^*z^*$ and $X=X^*+\Delta$.
By the above inequality, we have
\begin{align*}
\lVert U_r^*U_r^{*T}-U_rU_r^T\rVert_F & \leq\frac{\Delta_F+C(k,r)\lVert\Sigma_{>r}(X)\rVert_F}{\sigma_r(X^*)-\sigma_{r+1}(X_k)}\\
&\leq\frac{\Delta_F+C(k,r)\lVert\Sigma_{>r}(X)\rVert_F}{\sigma_r(X^*)-(\sigma_{r+1}(X^*)+\lVert \Delta\rVert_F+C(k,r)\lVert \Sigma_{>r}(X)\rVert_F)}\\
&\leq\frac{\Delta_F+C(k,r)\lVert\Sigma_{>r}(X)\rVert_F}{\sigma_r(X^*)-(\sigma_{r+1}(X^*)+\lVert \Delta\rVert_F+C(k,r)\lVert \Sigma_{>r}(X)\rVert_F)}
\end{align*}
where $U_r$ comes from $X$, $U^*$ comes from $X^*$.
\item The second inequality comes from Weyl, and the third inequality from the inequality we acquired above.
\item Since $n$ is large, this can be costly; some variants to do this faster.
\begin{enumerate}
\item Structured random matrix; apply \href{https://www.mathc.rwth-aachen.de/~rauhut/files/LinzRauhut.pdf}{DFR decomposition} on G, and we can calculate $AG$
 in better time complxity (this involves FFT)
\item Interpolation; use \href{https://docs.oracle.com/en/database/oracle/machine-learning/oml4sql/21/dmcon/cur-matrix-decomposition.html#GUID-9C3BF153-3A2B-4B4B-B711-D48976806799}
{CUR decomposition}.
\item Verify if the matrix has certain structures (like Toeplitz).
\item Process by blocks.
\item Adaptive methods; start with a small rank and increase it adaptively.
\item Perform SVD on a smaller matrix.
\item Power iterations algorithm.
\end{enumerate}
\end{itemize}

\section{Kernel trick}
\begin{itemize}
\item Imagine
\[
\min\limits_w f(\langle w,\psi(x_1)\rangle,\dots,\langle w,\psi(x_m)\rangle)+R(\lVert w\rVert)
\]
where $f:\mathbb{R}^m\to \mathbb{R}$ is an arbitrary function, $R:\mathbb{R}_+\to\mathbb{R}$ is a monotonically nondecreasing function, and 
$\psi$ is a mapping from $\mathcal{X}$ to a Hilbert space.
\begin{theorem}(Representer Theorem)
$w=\sum^m_{i=1}\alpha_i\psi(x_i)$ for some $\alpha\in\mathbb{R}^m$ gives an optimal solution.
\begin{proof}
Use the property of Hilbert space, namely the orthogonalization based on the subspace. When use the monotonicity of $\lVert w\rVert$.
\end{proof}
\end{theorem}
\item Note that $\psi$ is an intereseted embedding function to the higher dimensional space.
\begin{proof} 
Use the fact that Hilbert space has a basis (due to Gram-Schmidt), and express $w^*=\sum\limits^m_{i=1}\alpha_i\psi(x_i)+u$ where $w^*$ is an optimal solution. 
We have, by construction, 
\[
\lVert\sum\limits^m_{i=1}\alpha_i\psi(x_i)\rVert\leq \lVert w^*\rVert
\]
On the other hand, we have 
\[
f(\langle w,\psi(x_1)\rangle,\dots,\langle w,\psi(x_m)\rangle)=f(\langle w^*,\psi(x_1)\rangle,\dots,\langle w^*,\psi(x_m)\rangle)
\]
Combining these, it follows that $w$ gives an optimal solution.
\end{proof}
\item Now substituting this formula, we have 
\[
\min\limits_{\alpha\in\mathbb{R}^m}f(\sum\limits^m_{j=1}\alpha_jK(x_j,x_1),\dots,\sum\limits^m_
{j=1}\alpha_jK(x_j,x_m))+R(\sqrt{\sum\limits^m_{i,j=1}\alpha_i\alpha_jK(x_j,x_i)})
\]
where $K(x,x')=\langle \psi(x),\psi(x')\rangle$
\item Note that we now only need to calculate the value of $K$.
\begin{definition}
$G_{i,j}=K(x_i,x_j)$ is called Gram matrix.
\end{definition}
\begin{example}
Consider $k$ degree polynomial kernel defined to be $K(x,x')=(1+\langle x,x'\rangle)^k$. Then we have
\[
K(x,x')=\sum\limits_{J\in\{0,1,\dots n\}^k} \prod\limits^k_{i=1}\ x_{J_i}\prod\limits^k_{i=1}x'_{J_i}
=\langle \psi(x),\psi(x')\rangle
\]
by putting 
\[
\psi(x)=[\prod x_{J_{1,i}},\prod x_{J_{2,i}},\dots, \prod x_{J_{(n+1)^k,i}}]
\]
\end{example}
\begin{example}
Let $K(x,x')=\exp(-\frac{\lVert x-x'\rVert^2}{2\sigma})$.
By setting $\psi(x)=\frac{1}{\sqrt{n!}}\exp(-\frac{x^2}{2})x^n, n=0,1,\dots$, we have
\[
\langle \psi(x),\psi(x')\rangle=\exp(-\frac{\lVert x-x'\rVert^2}{2})
\]
\end{example}
\item In gerneral, symmetric $K$ is Kernel if and only if the associated gram matrix is positive semidefinite.
\item For a given Kernel, we usually perform eigenvalue decomposition, which leaves us with infinite dimensional space, 
which is not suitable computationally.
\item As a precursor, in order to decopose Kernel, we use the philosophy of interpolative decomposition (which doesn't work quite well for sparse matrix)
 and the detail of the kernel approximation is explained \href{https://arxiv.org/pdf/2006.02545.pdf}{here}.
\item Polynomial approximation to kernel can be found \href{https://arxiv.org/pdf/1109.4603.pdf}{here}.
\end{itemize}

\subsection*{Kernel approximation}
\begin{itemize}
\item We want to approximate a kernel $K:\Omega\times\Omega\to\mathbb{R}$. Let $[b_1,b_2,\dots,b_m]$ be polynomial basis, with each $b_i\in\mathbb{R}^{\infty\times m}$.
\item To do that: say $\lVert K(x,y)-\sum\limits^m_{i,j=1}b_i(x)\tilde\alpha_{ij}b_j(y)\rVert_{L^2(\Omega\times\Omega)}\leq\epsilon^m_A$, an approximation error. Try to find $\tilde\alpha$ such that
\[
K\approx B\tilde \alpha B^T
\]
\item Suppose $B$ satisfies $\langle b_i,b_j\rangle=\delta_{ij}$. 
\item We can choose $\hat\alpha$ by truncating $B$ to finite dimension. This procedure involves integration error. That is, 
\[
\tilde\alpha=\langle b_i, (Kb_j)\rangle\approx\sum_{l=0}^{m}b_i(q_l)(Kb_j)(q_l)w_l+\epsilon_I=\hat\alpha_{ij}+\epsilon_I
\]
\item It follows that 
\[
\lVert K-B\hat \alpha B^T\rVert_{L^2}\leq \epsilon_A+\epsilon_I
\]

Let $K=K_m+E$. We have
\begin{align*}
\hat\alpha &= \sum\limits^m_{l=0}b_i(q_l)(Kb_j)(q_l)w_l\\
&=\sum\limits^m_{l=0}b_i(q_l)(K_mb_j)(q_l)w_l+b_i(q_l)(Eb_j)(q_l)w_l
\end{align*}
Note that
\[
\sum\limits^m_{ij=1}b_i(q_l)(Eb_j)(q_l)w_l
\]
is bounded by approximation error scaled by m since $E=K-K_m$.
\item All in all, what we do is to 1) find a basis B (approximation error) and 2) pick a 
weight corresponding to nodes (integration error).
\item In the context of linear algebra, what we did is to 
\begin{enumerate}
\item First approximate by picking B: $B\tilde \alpha B^T=K$
\item approximate the corresponding $\alpha$ using quadrature and 
weight: $\bar B\hat \alpha\bar B^T=\bar K$ where 
$\bar B=[\sum_l b_{i,j}(q_l)]$ and $\bar K=[K(q_i,q_j)]$
\end{enumerate}
\item We can approximate Gaussian kernel by polynomials and get an error $m\sim O(1/\epsilon)$. This seems to involve the smoothness of the kernel though. 
In fact, we can impose less regularity on Kernel, which motivates CUR. Notice also the form $\bar B\hat \alpha\bar B^T$, which reminds us of CUR decomposition.
\end{itemize}

\section{Connection to CUR}
\begin{itemize}
\item Let's deviate a bit. In CUR decomposition, what happens if C and R are degenerate? The following existential theorem gives the answer for the worst case.
\begin{theorem}[\href{https://www.sciencedirect.com/science/article/pii/S0024379596003011}{ref}]
Let $A=A_r+F$ where $rank(A_r)\leq r$ and
$\lVert F\rVert_2\leq \epsilon=\sigma_{r+1}(A)$.
Then there exists $r$ column rows index I, J, and coefficient matrix G with dimension $r\times r$ such that
\[
\lVert A-CGR\rVert_2\leq \epsilon(1+\sqrt{\lVert \hat U^{-1}\rVert_2}+
\sqrt{\lVert \hat V^{-1}\rVert_2})
\]
where $A_r=U\Sigma V^T$, $\hat U=U(I, \cdot)$, and $\hat V=V^T(J,\cdot)$.
\end{theorem}
\begin{proof}
\begin{align*}
  A_r-CGR&=(U U^T + U_{\perp} (U_{\perp})^T)(A_r-CGR)(V V^T + V_{\perp} (V_{\perp})^T)\\
  &= 
\end{align*}
\end{proof}
\item The following proposition gives a further refinement of this bound.
\begin{proposition}
Let $I$ be chosen such that $|det(U(I))|$ is maximized for $U\in\mathbb{R}^{m\times r}$. Then
\[
\frac 1{\sigma_{\min}(U(I,\cdot))}\leq\sqrt{r(m-r)+1}
\]
\end{proposition}
\begin{proof}

\end{proof}
\item Going back to what we needed for performing CUR decomposition, we require,
\begin{enumerate}
\item find top singular vectors $U,V$ (in an attempt to get a better error bound).
\item find $I,J$ via maximum volume on $U,V$ (in an attempt to minimize the inverse of singular value). 
\end{enumerate}
\item Even with the cost, CUR has interpretability advantage since we're using the column verbatim.
\item With RSVD, the first procedure still takes $O(n^2)$; Second procedure is also combinatorially hard.
\item We can bypass the second one by using greedy algorithm ("good" locally). 
\item We want to avoid computing $U$ and $V$. Can we do that?
\begin{theorem}
Consider 2 by 2 block matrix $\{A_{ij}\}$. Suppose that $A_{11}$ has maximal value over all
r by r principal submatrix of A. Then 
\[
\lVert A_{22}-A_{21}A_{11}^{-1}A_{12}\rVert_{\infty}\leq (r+1)\sigma_{r+1}(A)
\]
\end{theorem}
\begin{proof} (sketch)
Let us prove in the case where $A_{21}, A_{22}$ and $A_{12}$ are one by one.
We have $A^{-1}=\frac 1{det(A)}adj(A)$. Define $(r+1)\times(r+1)$ matrix
$A_2 = 
\begin{pmatrix}
A_{11} & a_{12}\\
a_{21} & a_{22}
\end{pmatrix}
$. Since $A_{11}$ has maximum determinant, maximum value is achieved in $(r+1,r+1)$ index. It follows that
\[
\sigma_{r+1}(A)^{-1}=\lVert A^{-1}\rVert_2\leq (r+1)|A^{-1}_{r+1,r+1}|=(r+1)|a_{22}-a_{21}A^{-1}_{11}a_{12}|^{-1}
\]
Rearranging the term, we get the result.
\end{proof}
\item Finding the maximum submatrix is NP hard problem, so let us address this by usign greedy algorithm.
\end{itemize}

\section{Greedy Adaptive Cross Approximation with complete pivot}
\begin{enumerate}
\item \textbf{Input:} Initialization of matrix $A$.
\item \textbf{Output:} Sets $I$ and $J$.
\item \textbf{Initialize:} $I = J = \emptyset$, $R_0 = A$.
\item \textbf{Iterate:} 
\begin{enumerate}
\item \textbf{Selection step:} Compute 
\[
(i_k, j_k) = \arg\max_{i \not\in I, j \not\in J} |R_{k-1}(i, j)|
\]
\item \textbf{Update step:} Update the sets $I$ and $J$:
\[
I = I \cup \{i_k\}, \quad J = J \cup \{j_k\}
\]
\item \textbf{Pivot step:} Set the pivot value $p_k$:
\[
p_k = R_{k-1}(i_k, j_k)
\]
\item \textbf{Matrix update:} Update the matrix $R_k$:
\[
R_k = R_{k-1} - \frac{1}{p_k}R_{k-1}(\cdot, j_k)R_{k-1}(i_k, \cdot)
\]
\end{enumerate}
\item \textbf{Repeat:} until some stopping criterion is met.
\end{enumerate}
\begin{itemize}
\item Heuristically, the last step is like subtracting rank-1 approximation.
\end{itemize}

\section{Intuition and Analysis of GACA}
\textbf{(i) intuition}
\begin{itemize}
\item This corresponds to LU factorization with pivoting.
\item Related to CUR decomposition, let us assume that $A=A(\cdot, J)A(I,J)^{+}A(I,\cdot)+
\begin{pmatrix}
0 & 0\\
0 & A^{(k)}
\end{pmatrix}$.
Let  
$A=\begin{pmatrix}
A_{11} & A_{12}\\
A_{21} & A_{22}
\end{pmatrix}$ and $A_{11}=L_{11}U_{11}$. By simply 
rewriting the above formula, it follows that 
\[A=
\begin{pmatrix}
L_{11} & 0\\
L_{21} & I
\end{pmatrix}
\begin{pmatrix}
I & 0\\
0 & A^{(k)}
\end{pmatrix}
\begin{pmatrix}
U_{11} & U_{12}\\
0 & I
\end{pmatrix}
\]
where $A^{(k)}=A_{22}-A_{21}A_{11}^{-1}A_{12}, L_{21}=A_{21}U_{11}^{-1}$ and $U_{12}=L_{11}^{-1}A_{12}$
\item Therefore, if we stop at step k, we can pull out LU factorization of it. 
\item The crucial part is that we do not need to repeat LU factorization for $A_{11}$ for each iteration.
\item We can use the above equality to show that finding $(i_k,j_k)$ in the greedy procedure
 is same as picking $\arg\max\limits_{i\not\in I,j\not\in J}\det(A(I\cup i, J\cup j))$; just note that 
\[
det(A(\tilde I, \tilde J))=det(L_{11})det(U_{11})\lvert A^{(k)}(i,j)\rvert
\]
\item This does not guarantee the global maximization of the volume of the submatrix, but we're picking the best possible index
 in each iteration(adding one more column and row).
\end{itemize}
\textbf{(ii) analysis}
\begin{definition}
$\rho_r = \sup_{\text{rank(A)$>$r}}\frac
{\lVert A^{(r)}\rVert_\infty}{\lVert A\rVert_\infty}$ is called growth factor.
\end{definition}
\begin{theorem}
Let $A\in\mathbb{R}^{n\times n}$ be at least rank $r$. Then 
\[
\lVert A-A(\cdot, J)A(I,J)^{-1}A(I,\cdot)\rVert_{\infty}\leq 4^r \rho_r\sigma_{r+1}(A)  
\]
\end{theorem}
\begin{itemize}
\item Usually, $\sigma_{r+1}(A)$ decreases exponentially.
\begin{proof}
Let $I=\{1,\dots,r\}$ and $J=\{1,\dots,r\}$. Let $A_{11}=A[\tilde I, \tilde J]$.
As before, we have 
\[A=
\begin{pmatrix}
\tilde L_{11} & 0\\
\tilde L_{21} & I
\end{pmatrix}
\begin{pmatrix}
I & 0\\
0 & A^{(r+1)}
\end{pmatrix}
\begin{pmatrix}
\tilde U_{11} & \tilde U_{12}\\
0 & I
\end{pmatrix}
\]
The key step involves using lemma related to the above equality, and use induction to get the diagonal elements of $L_{11}$ and $U_{11}$ (and their characteristics, namely that the diagonal elements of $L_{11}$ is maximum in its column).
We then use the lemma by Higham which gives the crucial bound, $\lVert A_{11}^{-1}\rVert\leq 4^r\min\{\lvert p_1\rvert,\dots,\lvert p_{r+1}\rvert\}^{-1}$. The rest is to use the definition of growth factor.
\end{proof}

\item Due to maximization step, we generally get $O(rn^2)$ complexity.
\item With additional assumption for positive semi definite matrix, we have $O(rn)$.
\end{itemize}

\section{Compressed Sensing}
\begin{itemize}
\item Motivation: Say $y=Ax$, an underdetermined problem. Our goal is to compressed into 
$y$ and reconstruct $x$. Two main question arises:
\begin{enumerate}
\item What matrices A should we use?
\item How can we reconstruct?
\end{enumerate}
\item The problem at hand is that we do not know the structure of $x$ at hand. For example we only know that it is sparse, but doesn't quite
 know exactly the exact index.
\item So to answer your question, one reason we might use 
the Fourier transform in compressed sensing is to transform 
the signal into a domain where its sparsity is more apparent, 
making it easier to find a sparse representation of the signal.
\end{itemize}

\section{Compressed Sensing (2)}
\begin{itemize}
\item This method juxtaposes PCA method.
\begin{definition}
A matrix $W\in \mathbb{R}^{n,d}$ is $(\epsilon, s)-RIP$ (with $\epsilon<1$) if for all $x\neq 0$ s.t. $\lVert x\rVert_0\leq s$
 we have 
 \[
|\frac{\lVert Wx\rVert^2_2}{\lVert x\rVert^2_2}-1|\leq \epsilon
 \]
\end{definition}
\begin{theorem}
If $W\in\mathbb{R}^{n,d}$ is an $(\epsilon, 2s)\sim RIP$ matrix, 
and if $x$ has less than $s$ many nonzero elements, then $x$ is the unique sparsest vector that
 gets mapped to $Wx$ by the matrix $W$.
\end{theorem}
\begin{proof}
Assume $\tilde x\neq x$. Observe that 
$\lVert x-\tilde x\rVert\leq 2s$ and $W(x-\tilde x)=0$. 
On the other hand, $\lvert 0-1\rvert\leq \epsilon$, contradicting 
the definition of RIP matrix.
\end{proof}
\item This implies that, for specific compression matrices and sparse data, the original data 
can be accurately recovered.
\begin{theorem}
Let $\epsilon<\frac 1{1+\sqrt{2}}$ and let $W$ be a ($\epsilon, 2s$)-RIP matrix. Let
 $x$ be an arbitrary and let $x_s$ be the vector which equals $x$ for the $s$ largest
  elements of $x$ and equals 0 elsewhere. Let $x^*\in \arg\min\limits_{v:Wv=y}\lVert
    v\rVert_1$. Then
\[
\lVert x^*-x\rVert_2\leq 2\frac{1+\rho}{1-\rho}s^{-1/2}\lVert x-x_s\rVert_1
\]
where $\rho=\sqrt{2}\epsilon/(1-\epsilon)$.
\end{theorem}
\begin{remark}
In particular, we have 
\[
x=\arg\min\limits_{v:Wv=y}\lVert
v\rVert_0=\arg\min\limits_{v:Wv=y}\lVert v\rVert_1
\]
for $\lVert x\rVert_0\leq s$.
\end{remark}
\item We used $L^1$ since we are looking for
solutions that are "almost sparse" rather than strictly sparse. Further, 
$L^1$ is convex so we can compute efficiently.
\begin{theorem}
Let $\epsilon, \delta\in(0,1)$.
Let $U$ be orthonormal matrix of size $d\times d$. Further, 
let $W\in\mathbb{R}^{n,d}$ be generated by $N(0,1/n)$ where 
$n\geq 100\frac{s\log(40d/(\delta\epsilon))}{\epsilon^2}$ and $s\in [d]$. Then the matrix
$WU$ is $(\epsilon,s)$-RIP.
\end{theorem}
\begin{remark}
This is useful when the sparsity is hidden, i.e. $y=U\alpha$ where $y$ is sparse.
\end{remark}
\item Connection of PCA with compressed sensing: While PCA identifies the dominant 
subspace in which most of the data's energy or variance lies, compressed sensing 
exploits the fact that signals often have sparse representations in some domain. 
These two concepts are related in the sense that they both exploit inherent structures
in data (low-rank structure and sparsity, respectively) for efficient processing 
or recovery.
\item As another note, we can also exploit the rank of the data matrix.
\end{itemize}

\section{Compressed Sensing (3)}
\begin{itemize}
\item We need a number of points exponential in $d$ in the approximation scheme of Kernel.
\item We are only solving for $[\alpha_{ij}]_{i,j}$, which are $m^2$ coefficients, maybe we may require less points in some situation.
\item Let $A_{ij}=b_j(q^{(i)})$ and $y_i=f(q^{(i)})$, and we want to solve $y_i=A\alpha$.
\item in general, $N,p$ have exponential scaling with respect to the dimension.
\item The situation we're looking for is when $\alpha$ has "low complexity."
\item Let $\alpha^*$ is s-sparse with $supp(\alpha^*)=S$. 
To determine $\alpha^*\in\mathbb{R}^d$, we need for A to satisfy RNP (restricted null space property), that is, 
\[
null(A)\cap\{\Delta\in\mathbb{R}^p|\lVert\Delta_S\rVert_1\geq\lVert\Delta_{S^C}\rVert_1\}=\{0\}
\]
\begin{theorem}
Let $\alpha^*$ be s-sparse and we have $A\alpha=y$.
Solving 
\begin{align*}
\min \lVert \alpha\rVert_1 \text{ such that } A\alpha=y
\end{align*} has a unique solution $\alpha$ iff $A$ satisfies RNP.
\end{theorem}
\begin{proof}
For backward direction, it suffices to show that $\{\alpha|\lVert\alpha\rVert_1\leq \lVert\alpha^*\rVert_1, A\alpha=y\}
=\{\alpha^*\}$. The idea is to set $\Delta=\tilde\alpha - \alpha^*$ and show that $\Delta=0$ using RNP.
For forward direction, let $x\in null(A)$ and notice that $Ax_S=-Ax_{S^C}$, and so by uniqueness 
$\lVert x_{S^C}\rVert_1>\lVert x_S\rVert_1$. It follows that $x=0$. This means that A satisfies RNP.
\end{proof}
\item In practice, people usually engineer such $A$ to find $\alpha$.
\item Sufficient conditions for RNP
\begin{enumerate}
\item Pairwise incoherence of A
\[
\lVert A_S^T A_S-I\rVert_{\infty}< \frac 1{2s}, \forall\lvert S\rvert\leq s
\]
\begin{proof}
Let $\alpha=\alpha_S+\alpha_{S^C}$ where $\alpha\in null(A)$. So $A(\alpha_S+\alpha_{S^C})=0$
, that is, $A\alpha_S=-A\alpha_{S^C}$.
By algebraic manipulation, we get
\[
\lVert \alpha_S\Vert_1\leq\dfrac{s\delta}{1-s\delta}\lVert\alpha_{S^C}\rVert_1
\] Since $\delta < \frac 1{2s}$, we have $\lVert\alpha_S\rVert_1<\lVert\alpha_{S^C}\rVert_1$. 
We showed that if $\alpha\in null(A)$, then $\lVert \alpha_S\rVert_1<\lVert\alpha_{S^C}\rVert_1$. So $A$ satisfies RNP.
\end{proof}
\begin{itemize}
\item Asking for something more than $S$ might be too much, so we restrict it to $S$.
\item This $A^T A$ situation in general comes up in regression 
problem; in $A^T Ax=A^T y$ we want $A^T A$ to be close to identity in order to recover $x$.
\item By JL lemma, we have $\lVert\dfrac{G^T G}{N}-I\rVert_{\infty}\leq\sqrt{\dfrac{\log p}N}=\epsilon$.
\item This is because in JL lemma, we have $N-O(\dfrac{\log p}{\epsilon^2})$. This achieves $N\sim s^2$, but in general, we want $N\sim s$.
\end{itemize}
\item Restricted Isometry property: let $\delta_s=\max\limits_{\lvert S\rvert=s}\lVert 
A^T_S A_S-I\rVert_2$. We require $\delta_{2S}< \frac 13$ to have a unique recovery.
\begin{lemma}
Let supp(u)=S, supp(v)=T and $supp(u)\cap supp(v)=\phi$. 
Then we have $\lvert\langle Au, Av\rangle\rvert\leq\delta_{s+t}\lVert u\rVert_2\rVert 
v\rVert_2$
\end{lemma}
\begin{proof} (lemma)
Use the fact that $\lvert \langle A_S u_S, A_S 
v_S\rangle\rvert=\lvert\langle(A_S^T A_S-I)u_S, v_S\rangle\leq 
\lVert A_S^T A_S-I\rVert_2\lVert u_S\rVert_2\lVert v_S\rVert_2$.
\end{proof}
\begin{proof} (theorem)
Let $\alpha$ satisfies $A\alpha=0$. We want to show $\lVert \alpha_S\rVert_1<\lVert 
\alpha_{S^C}\rVert_1$, or equivalently, $\lVert \alpha_S\rVert_1<\frac 12\lVert 
\alpha\rVert_1$. To this end, we may reorder $\alpha$ so that the first s entires are the biggest s elements, and the next s entires are the next
 biggest s entries, and so on.
The key step is as follows. First note that  
\begin{align*}
\lVert\alpha_{S_0}\rVert^2 &\leq \frac 1{1-\delta_{2S}}\lVert A\alpha_{S_0}\rVert^2
\end{align*}
We use the lemma to arrive $\leq\dfrac{\delta_{2s}}{1-\delta_{2s}}\lVert\alpha_{S_0}\rVert\sum\limits_{2k\geq 1}\lVert \alpha_{S_k}\rVert_2$.
Further, the construction of $\alpha$ yields $\leq \dfrac{\delta_{2S}}{1-\delta_{2S}}\lVert\alpha_{S_0}\rVert_2\sum\limits_{k\geq 1}\dfrac{\lVert\alpha_{S_{k-1}}\rVert}{\sqrt{s}}$.
\end{proof}
\end{enumerate}
\item The Iterative Hard Thresholding algorithm is given by the following steps:
\begin{align}
    a_{t+1} &= z_t - A^T(Az_t - y), \\
    z_{t+1} &= P(a_{t+1}),
\end{align}
where $P(a_{t+1})=\min\limits_{z\text{ is s-sparse}}\lVert z-a_{t+1}\rVert$.
\item This works when we know that the solution is s-sparse. Refer to \href{https://users.math.msu.edu/users/iwenmark/Teaching/MTH994/Holger_Simon_book.pdf}{here}, page 76.
\item This is fast essentially because calculating $Az_t$ is fast.
\item The optimization process usually is by balancing two objects illustrated as follows
\[
z_{t+1}=\arg\min_z\langle z-z_t,\nabla f(z-z_t)\rangle+\frac 1{2\tau}\lVert z-z_t\rVert^2
\] 
The philosophy here is similar.
\item The convergence is shown to be linear:
\begin{lemma}
Under RIP assumption with $\delta_{3s}=\frac 18$, we have $\lVert z_s-z^*\rVert_2\leq 2^{-t}\lVert z^*\rVert_2$.
\end{lemma}
\end{itemize}

\section*{Relation to JL lemma}
\begin{itemize}
\item In JL lemma, we have non-dependency on $d$ essentially 
because we only needed to compute $N^2$ points...
\item Now we're interested in $s-$sparse vectors $z$, instead of 
all the $\binom{n}{2}$ paris, so we need some kind of notion of measure.
\item We can indeed emulate the proof of JL lemma to get JL type of inequality.
\item To this end, we start from
\begin{fact}
$P(\lVert Az\rVert^2_2-\lVert z\rVert^2>\epsilon\lVert z\rVert^2)\leq 2\exp(-N C_0(\epsilon))$
where $A=\dfrac G{\sqrt{N}}$ and $G_{ij}\sim N(0,1)$
\end{fact}
that we proved before.
\begin{lemma}
For given support $S$, $\dfrac{\lVert Az\rVert^2}{\lVert z\rVert^2}=1\pm\delta$ for
any supp(z)=S and $\lVert z\rVert_2=1$ with probability $1-2(\dfrac{12}\delta)^s\exp(-N C_0(\dfrac{\delta}2))$
\end{lemma}
\begin{theorem}
For all $s$-sparse $z$ with $\lVert z\rVert=1$, we'll get a probability $1-2(\dfrac{cp}{s})^s(\dfrac{12}{\delta})^s\exp(-NC_0(\dfrac{\delta}2))$
\end{theorem}
\item The multiplicative term is called covering number. Logarithm of it gives us a dimensionality.
\end{itemize}
\section*{Matrix Completion}
\begin{itemize}
\item We approximate $K(x,y)\approx\sum_{i,j}\alpha_{ij}b_i(x)b_j(y)$. 
How many points do we need to get, say $\epsilon$ error?
\item This is like recovering $M\in\mathbb{R}^{n_1\times n_2}$ matrix from a submatrix drawn from $M$.
\item We assume ``with replacement."
\item We may assume that M satisfies the following.
\begin{enumerate}
\item $M$ is of rank $r$.
\item $\mu(U)=\dfrac{n_1}r\max\limits_{1\leq i\leq n_1}\lVert U^T e_i\rVert^2_2\leq \mu_0$ (this measure how spread 
the entries of singular vectors are, and is called "incoherence condition" since it
essentially measure how incoherent it is to the canonical vector)
\item $\mu(V)\leq \mu_0$
\item Each entry of $UV^T$ satisfies
\[
\lVert UV^T\rVert_{max}\leq\mu_1\sqrt{\dfrac{r}{n_1n_2}}
\] (this says that moreover, $UV^T$ should also have spreadout entries, this is a bit stronger than the previous condition)
\end{enumerate}
\item These conditions basically rule out interpolating with non-smooth highly concentrated basis.
\item Heuristically the second condition signifies the spreadout of the columns and the last condition talks about the spreadout
across the rows as well.
\begin{theorem}
Under these assumptions, if $m=\lvert\Omega\rvert\geq\max\{\mu_1^2, \mu_0\}\cdot r(n_1+n_2)\beta\log^2(2n_2)$ for 
some $\beta>1$ then the solution for $\min_X\lVert X\rVert_*\quad\text{s.t. }X_{ij}=M_{ij}, \forall (i,j)\in\Omega$ is $M$ with 
high probability.
\end{theorem}
\begin{proof}
Revisit RNP: we wanted to show that if $A\Delta=0$ and $\Delta\neq 0$, then we necessarily have $\lVert \Delta_{S^C}\rVert>\lVert \Delta_S\rVert$.
We then have $\lVert x^*+\Delta\rVert_1>\lVert x^*\rVert_1$.

A little deviation: we call $v$ is subgradient if
\[
\forall u\in S, f(u)\geq f(w)+\langle u-w, v\rangle
\]
Existence of subgradient is simply the reformulation of convex function.

\end{proof}
\end{itemize}
\section*{Random Sampling in Bounded Orthonormal Systems}
\begin{itemize}
\item Bounded orthonormal system is an orthonormal system with
\[
\lVert \phi_j\rVert_\infty:=\sup_{t\in\mathcal{D}}\lvert \phi_j(t)\rvert\leq K, \forall j\in [N]
\]
\item Smallest such K is 1.
\end{itemize}
\end{sloppypar}
\end{document} 