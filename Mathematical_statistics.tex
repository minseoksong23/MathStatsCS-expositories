\documentclass[11pt,reqno]{amsart}
\usepackage{graphicx, url}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{titlesec}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\graphicspath{ {./images/} }
\usetikzlibrary{shapes}
\usepgfplotslibrary{polar}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{backgrounds}
\pgfplotsset{every axis/.append style={
                    axis x line=middle,    % put the x axis in the middle
                    axis y line=middle,    % put the y axis in the middle
                    axis line style={<->,color=blue}, % arrows on the axis
                    xlabel={$},          % default put x on x-axis
                    ylabel={$},          % default put y on y-axis
            }}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\usepackage[utf8]{inputenc}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{fact}{Fact}
\newtheorem{remark}[example]{Remark}

\newcommand{\bigsection}[1]{
  \titleformat*{\section}{\centering\LARGE\bfseries}
  \section*{#1}
  \titleformat*{\section}{\large\bfseries} % Reset to the original format
}
\titleformat{\section}
  {\normalfont\Large\bfseries\centering}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\large\bfseries}{\thesubsection}{1em}{}

\title{Mathematical statistics}
\author{MinSeok Song}
\date{}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\begin{document}
\maketitle
\bigsection{Mathematical Statistics}
\section{Sufficiency and Exponential family}
\begin{definition}
Statistic T is called sufficient if $X\mid T$ does not depend on $\theta$.
\end{definition}
\begin{itemize}
\item ($\ast$)We need \textbf{measure theoretic} definition of sufficiency. What 
exactly is meant that samples from $X\mid T$ follow the same distribution as $X$?
\item ($\ast$)What I am getting at is that $P(X\in A)=P(X\in A\mid \sigma(w:T(X(w))\in T(A)))(w)$ for $w\in \{w:T(X(w))\in T(A)\}$. To be continued (check!)
\item Well, I think it is a direct consequence of $P(X=x)=\int P(X=x\mid T(x)=t)P(T(x)=t)dt$. So it indeed follows from definition. 
\item Here, $T$ being sufficient is irrelevant. However, we'll easily sample from $P(X=x\mid T(x)=t)$ because the distribution $X\mid T$ doesn't depend on $\theta$.
\item The setting here is that we are trying to infer $\theta$ based on data $X$. Bayesian 
definition would be phrased as $X\mid T \perp \theta$
\item The intuition here is that having $X$ given $T$ does not give further information about $\theta$. 
Just by definition, $X$ ($n$ samples) has the same distribution as a sample from $X|T$ (also $n$ samples).
\begin{example}
\item For example $X|T$ has a distribution depending on $T(X)$ such as $$N(1^T \bar X, \begin{pmatrix}
1-1/n & -1/n & \dots \\
-1/n & 1-1/n & \dots \\
\dots & & 1-1/n
\end{pmatrix})$$ and if we sample $\tilde X_i$'s from this distribution, it does follow $N(\theta,1)$ (the conditional distribution 
follows that above distribution).
\item Another example would be sum for Bernoulli, and maximum value for uniform distribution. 
\item By the way, maximum and minimum of samples from uniform distribution are both sufficient, illustrating that 1d
sufficient statistic is not necessarily minimal. 
\end{example}
\item Any one to one function of minimal sufficient statistic is also minimal sufficient statistic. For example, $(T(x),T(x))$ would do.
\item Lehmann-Scheffe theorem establishes the relationship between unbiasedness and sufficiency of statistic.
\end{itemize}
\begin{theorem}(factorization theorem)
Sufficient statistic $T$ satisfies $P(x)=f_\theta(T(x))g(x)$, that is, $\theta$ depends only through $T(x)$.
\end{theorem}
\begin{definition}
A sufficient statistic $T$ is called minimal if any other sufficient statistic $M$ is a function of $T$.
\end{definition}
\begin{lemma}
Suppose $H_0\subset H$. Suppose $S$ is minimal sufficient for $P_\theta,\theta\in H_0$ and sufficient for $P_\theta,\theta\in H$. Then 
it is minimal sufficient for $P_\theta,\theta\in H$.
\end{lemma} 
\begin{itemize}
\item This is because any sufficient statistic in a larger parameter space is 
sufficient in a smaller set  of parameter space by definition.
\end{itemize}
\begin{theorem}
For $P_\theta:\theta\in\{\theta_1,\dots,\theta_d\}$, $T(x)=(\frac{P_{\theta_1(x)}}{P_{\theta_0}(x)}, 
\frac{P_{\theta_2(x)}}{P_{\theta_0}(x)},\dots \frac{P_{\theta_d(x)}}{P_{\theta_0}(x)})$ is a minimal sufficient statistic.
\end{theorem}
\begin{proof}
Sufficiency by definition. Minimality due to factorization theorem.
\end{proof}
\begin{definition}
\textbf{Exponential family} is of the form $p(x\mid \theta)=\exp(\sum\limits^d_{j=1}\eta_j(\theta)T_j(x)-B(\theta))h(x)$. 
\end{definition}
\begin{itemize}
\item We can see that $B(\theta)=\log\int e^{\sum^d_{j=1}\eta_j(\theta)T_j(x)}h(x)d\mu(x)$, which can be viewed as a function of $\eta$.
\item Canonical form is of the form $p(x\mid \eta)=\exp(\sum^d_{j=1}\eta_jT_j(x)-A(\eta))h(x)$.
 We can see that $A(\eta)$ is convex function and parameter space is convex set.
\item Using natural parameter and normalizing constant, we can easily get expected value and variance of $T(X)$, i.e. 
$E_\theta[T(x)]=J^{-1}\nabla B$.
\end{itemize}
\begin{theorem}(Rau-Blackwell)
Assume $L(\hat\theta,\theta)$ is convex in $\hat\theta$ for any $\hat\theta$ and any sufficient statistic $T$. Define
$\hat\theta=E_\theta(\hat\theta\mid T)$. Then $R(\hat\theta,\theta)\leq R(\hat\theta,\theta)$.
\end{theorem}
\begin{proof}
$L(\hat\theta,\theta)=L(E_\theta(\hat\theta\mid T),\theta)\leq E_\theta(L(\hat\theta,\theta)\mid T)$ by Jensen inequality. Take expected value on both sides and we get the result.
\end{proof}
\begin{itemize}
\item I don't think we used sufficiency in the proof?
\end{itemize}
\begin{definition}
Statistic $T$ is called ancillary if it doesn't depend on $\theta$. If the expected value doesn't depend on $\theta$, it's called first order ancillary.
\end{definition}
\begin{definition}
$T(X)$ is complete iff $E_\theta(f(T(x)))=0,\forall\theta\in H$ imples $f(T(x))=0$ almost surely. 
\end{definition}
\begin{itemize}
\item Ancillary statistic is something that is not informative about $\theta$.
\item In words, completeness means that if the function of statistic is first-order ancillary, it should better be constant.
\item So, since $T$ already has much information about $\theta$, if $f(T)$ has hint of having no information about $\theta$ (first order ancillary), it surely doesn't 
have information about $f(T)$.
\item Few ways to prove completeness: 
\begin{enumerate}
\item use the idea of MGF
\item use measure theory result: if the integral on measurable set is same then the integrand is same almost everywhere,
\item For full rank exponential family, $T$ is complete. 
\end{enumerate}
\item In order to complete non-completeness, one uses counter-exmple of non-zero $f$. One uses symmetry; for example 
one may use $\sum_{i=1}^n x_{(i)}$-median.
\end{itemize}
\begin{theorem}(Bahador)
If T is sufficient and complete, then T is minimal sufficient.
\end{theorem}
\begin{itemize}
\item T may well be minimal sufficient but not complete. 
\end{itemize}
\begin{theorem}(Basu)
If T is sufficient and complete, and if A is ancillary, then T and A are independent.
\end{theorem}
\begin{itemize}
\item T is like a perfect summary of the parameter, while A doesn't contain information on $\theta$ so it makes sense
that T and A are independent of each other.
\end{itemize}
\section{Decision Theory}
\begin{definition}
The risk is defined by $E_\theta(l(\hat\theta(X),\theta))$. It is a function of $\theta$.
\end{definition}
\begin{itemize}
\item If we have a prior on $\theta$, we can further define Bayes estimator, that is, 
$$
\arg\min_{\hat\theta(X)\in H}\int R(\hat\theta(X),\theta)\pi(\theta)d\theta
$$
\item Minimax is defined as $\arg\min_{\hat\theta(X)\in H}\sup_{\theta\in H}R(\hat\theta(X),\theta)$
\item Note that this is a function of $X$, that is, statistic. 
\end{itemize}
\begin{fact}
$\hat\theta_\pi(x)=\arg\min_a\int L(a,\theta)\pi(\theta\mid x)d\theta$ is Bayes.
\end{fact}
\begin{proof}
Fubini theorem.
\end{proof}
\begin{fact}
For a squared loss, Bayes estimator is given by $E(\theta\mid X)$. This is essentially due to bias-variance trade-off!
\end{fact}
\begin{fact}
$\arg\min_a E(\lvert X-a\rvert)=\text{median of X}$ where median is defined as $P(X\leq m)\geq 1/2$ and $P(X\geq m)\geq 1/2$.
\end{fact}
\begin{fact}
The median is given by a closed interval $[m_0,m_1]$.
\end{fact}
\begin{itemize}
\item The inequality is important, because we are excluding the edge case where for example $P(X=0)=2/3$ and $P(X=1)=1/3$.
\item The above fact is due to the following string of (in)equality, given $m_1<c$.
$$
E(\lvert X-c\rvert)-E(\lvert X-m\rvert)=(c-m)[P(X\leq m)-P(X > m)]+2\int_{m<x<c} (c-x)dP(x)\geq 0
$$
\item As you can see the right hand side is some kind of deviation term, and the left hand side used the definition of median.
\item $\arg\min_{\hat\theta(X)\in H} E_\theta(\lvert \hat\theta(X)-\theta\rvert)$ is given by $\hat\theta_\pi(x)=\arg\min_a\int \lvert a-\theta\lvert \pi(\theta\mid x)$, according to the previous argument, posterior median.
\end{itemize}
\begin{theorem}
If for some $\pi$, $\hat\theta$ satisfies $\sup_{\theta\in H}R(\hat\theta,\theta)=\inf_{\hat\theta}\int R(\tilde \theta,\theta)\pi(\theta)d\theta$, 
then $\hat\theta$ is minimax.
\end{theorem}
\begin{proof}
$\forall\tilde\theta$, 
\begin{align*}
\sup_{\theta\in H}R(\tilde\theta,\theta)&\geq\int R(\tilde\theta,\theta)\pi(\theta)d\theta\\
&\geq \inf_{\tilde\theta}\int R(\tilde\theta,\theta)\pi(\theta)d\theta\\
&=\sup_{\theta\in H}R(\hat\theta,\theta)
\end{align*}
\end{proof}
\begin{itemize}
\item This says that if worst possible risk for an estimator $\hat\theta$ is best possible average risk, then $\hat\theta$ is performing as well as possible, even in the worst-case scenario.
\end{itemize}
\begin{corollary}
If $\hat\theta=\hat\theta_\pi$ for some $\pi$ and $R(\hat\theta_\pi,\theta)$ is constant over $\theta\in H$ then $\hat\theta$ is minimax.
\end{corollary}
\begin{proof}
The first inequality becomes equality due to constant assumption, and the second inequality 
becomes equality due to $\hat\theta$ being Bayes.
\end{proof}
\begin{fact}
The minimax estimator for squared loss is unbiased.
\end{fact}




























\bigsection{DS procedure}
The dichotomy of Bayesian and Frequentist approach is mostly a matter of division based on foundational philosophical differences, and in some situation is not necessarily natural; each instance can be formally simply seen as a choice of analysis. The dichotomy has mainly developed based on two philosophical school of thoughts:
 subjective belief (via prior distribution) vs. long-run frequency (via sampling distribution).
\begin{enumerate}
\item Frequentist
\begin{itemize}
\item They avoid making probabilistic claims about model parameters and hypothesss. Instead, they describe the behavior of statistics and procedures over many hypothetical repeated samples.
\item They use Statistics derived from data, and use deterministic approach for inference, with methods like hypothesis testing and confidence intervals to draw conclusions about population parameters based on sample data.
\item examples: T-test, linear regression, etc
\end{itemize}
\item Bayesian
\begin{itemize}
\item They assign prior beliefs (distribution function) on parameters of model.
\item They use probabilistic argument on specific hypothesis or parameter values.
\item examples: MCMC, Bayesian hierarchical modeling, etc
\end{itemize}
\end{enumerate}
\section{Data Science Procedure}[list of topics that will be discussed]
\begin{enumerate}
\item Preparation of data
\begin{itemize}
\item handling missing data (imputation, etc)
\item transformation
\item Box-Cox transformation
\item Outliers
\end{itemize}

\item Interpret descriptive statistics about data
\begin{itemize}
\item plots
\item t-distribution
\item F-distribution
\end{itemize}

\item Modeling (parametric, nonparametric)
\begin{itemize}
\item ANOVA
\item Ensemble methods
\item Boosting
\item AIC-BIC
\item Regularization
\end{itemize}

\item Predict based on model
\begin{itemize}
\item Confidence/Prediction intervals
\item Real-time prediction
\end{itemize}

\item Inference on statistics
\begin{itemize}
\item Hypothesis testing
\item p-value
\item Type I/II error
\begin{enumerate}
\item Type I: reject null when true: for example, it's like wrongly assuming that the medication has an effect, when it doesn't.
\item Type II: fail to reject null when false: for example, it's like failing to deciding that the medication has an effect, when it has an effect. 
\end{enumerate}
\item Causal inference
\end{itemize}

\item Evaluation of models
\begin{itemize}
\item Cross-validation
\end{itemize}

\item Communication of models
\end{enumerate}

\subsection{Types of Missing Data}
\begin{enumerate}
\item MCAR
\item MAR
\item MNAR
\end{enumerate}


\section{Outliers}
\section{Inference on statistics}
\section{p-value}
\begin{definition}
Given some hypothesis, the p-value is the probability of obtaining test results at least as extreme as 
the result actually observed, under the assumption that the null hypothesis is correct.
\end{definition}
\begin{itemize}
\item If p-value is less than significance level, we reject the null hypothesis. Significance 
level is a threshold we set to decide when we have enough evidence to reject the null hypothesis in favor of the alternative hypothesis.
\item In statistics, we do not quite "prove" that the hypothesis is true. We can only reject the false statements or assertions (the theory of falsification, proposed by Karl Popper). 
\end{itemize}

\section{t-distribution}
\begin{definition}
t-distribution is defined as $\frac Z{\sqrt{\frac Vk}}$ where $Z \perp V$, $Z\sim N(0,1)$, and $V\sim \chi_k^2$.
\end{definition}
\begin{itemize}
\item The t-distribution has the thicker tails than the normal distribution.
\item Note that the $\chi_k^2$ distribution has mean $k$ and variance $2k$. Hence as $k$ gets larger the t-distribution 
looks like standard normal distribution due to CLT. 
\item In multiple regression setting, $\frac{\frac{\hat\beta_1-\beta_1}{\frac \sigma{\sqrt{\sum_i(X_i-\bar X)^2}}}}
{\sqrt{(n-2)\cdot\frac{\hat \sigma^2}{\sigma^2}/(n-2)}}\sim t_{n-2}$ under normal assumption of residual.
\end{itemize}

\section{Chi-squared distribution}
\begin{itemize}
\item It is possible to have $f(X)$ and $g(X)$ be independent. The key example concerns chi-distribution: Say $\theta$ is uniformly distributed in $[0,2\pi]$. 
Let us say $R^2$ follows $chi$-squared distribution with degree 2: this acts like a distance square in the 2d space. Then $X= r\cos(\theta)$ and $Y=r\sin(\theta)$ are independent.
\item The definition of chi-square looks contrived, in particular compared to normal distribution, but we use it a lot in the context of sum of squares.
\item Under moment assumption, it's approximately true for large $n$ via CLT.
\item Under moment assumption, it's approximately true for large $n$ via CLT.
\end{itemize}


\section{Prediction interval, Confidence interval}
\begin{itemize}
\item In the context of linear regression, a \textbf{confidence interval} provides a range of values within which we expect the true regression coefficient (e.g., $ \beta_0 $ or $ \beta_1 $) to lie with a certain level of confidence. This reflects our uncertainty about the 
value of the coefficient based on the data we have observed.
\item On the other hand, a \textbf{prediction interval} provides a range of values within which a new observation $ y $ (given a particular $ x $) is expected to fall with a certain level of confidence.
We have $y=\beta_0+\beta_1 x+\epsilon$. Note the additional uncertainty introduced by the random error $\epsilon$.
\end{itemize}

\subsection{Fisher information}
\begin{definition}
Fisher information is defined by $\mathcal{I}(\theta)=E[(\frac \partial{\partial\theta}\log f(X;\theta))^2|\theta]$ for appropriately regular $f$.
\end{definition}

\begin{itemize}
\item Note that $E[\frac{\partial}{\partial\theta}\log f(X;\theta)|\theta]=0$.
\item Think of this as how much (the logarithm of) likelihood varies in the vicinity of $\theta$.
\item Large variation of likelihood we have more "information." (for example, if a deviation causes a significant drop in likelihood, 
it means that the data is most probable under the original $\theta$ value.)
\end{itemize}

\begin{theorem} [Cramer-Rao bound]
Say $\hat\theta(X)$ is an unbiased estimator. We have $Var(\hat\theta)\geq \frac 1{\mathcal{I}(\theta)}$.
\end{theorem}

\begin{itemize}
\item This says that as the fisher information gets larger, we have better precision.
\item There is a similar result for biased estimator.
\end{itemize}

\section{Consistency}
\begin{definition}
We say $\hat\theta$ is consistent to $\theta$ if $P(\lvert \hat\theta-\theta\rvert<c)\to 1$ as $n\to\infty, \forall c>0$.
\end{definition}
\begin{example}
Say $X_i$ and $Y_i$ are sampled from $N(\mu_i,\sigma^2)$. We can see that MLE $\hat\sigma^2$ is not consistent because of growing
number of nuisance parameters. Essentially, as we add more groups and calculate $\hat\mu_i$, even if
we have more data, it doesn't help us get a better estimate of the variance $\sigma^2$ due to ``nuisance parameters" $\mu_i$'s.
\end{example}
\begin{itemize}
\item So how do we deal with it? There are two ways.
\begin{enumerate}
\item Integrate out all the nuisance parameters and use MLE on that.
\item Condition on the minimal sufficient statistic of nuisance parameters and maximize the conditional distribution.
\end{enumerate} 
\end{itemize}

\section{Location family}
\begin{definition}
It is a family of probability distribution parametrized by a location parameter $\mu$ and a non-negative scale parameter.
\end{definition}
\begin{itemize}
\item The distribution function is of the form $F(\frac{x-a}{b})$.
\item We can check that $(X_1-X_i)/b$ is ancillary when $X_i's$ are i.i.d. for a known $b$ because $b$ will be cancelled.
\item Elaboration: say $X\sim F(x)$ and $X=a+bY$. Then $P(X\leq x)=P(a+bY\leq x)=P(Y\leq\frac{x-a}b)=G(\frac{x-a}b)=F(x)$
\end{itemize}

\section{Uniformly most powerful test(UMP)}
\begin{definition}
A test function $\phi(x)$ is UMP of size $\alpha$ if for any other test function $\phi'(x)$ satisfying 
$$
\sup_{\theta\in \Theta_0} E(\phi'(X)\lvert \theta)=\alpha'\leq \alpha=\sup_{\theta\in\Theta_0}E(\phi(X)\lvert \theta)
$$
we have
$$
\forall \theta\in\Theta_1,\quad E(\phi'(X)\lvert \theta)=1-\beta'(\theta)\leq 1-\beta(\theta)=E(\phi(X)\lvert \theta)
$$
\end{definition}
\begin{itemize}
\item The first term is type I error, and the second term is the power of test function.
\item It says that UMP test has the highest power among all tests whose type I error is less than or equal to $\alpha$, a maximum type I error for our chosen test function.
\item The assumption is needed because we can artificially make the test function to have th highest power by rejecting all the time.
\end{itemize}

\section{unbiasedness of test function}
\begin{definition}
Define $\phi(X)=1$ if $X\in R$ and $0$ if $X\in R^c$ where $R$ is a reject region. If a test function $\phi(X)$ satisfies $E_\theta \phi(X)\leq \alpha$ if 
$\theta\in\Omega_H$ and $E_\theta \phi(X)\geq\alpha$ if $\theta\in\Omega_K$ then it is said to be unbiased.
\end{definition}
\begin{itemize}
\item UMP is unbiased. Think of a test function $\phi=\alpha$.
\end{itemize}

\section{distributions}
\subsection{Exponential distribution}
\begin{itemize}
\item The distribution function is $F_X(x)=1-e^{-\lambda t},t\geq 0$. $\lambda$ here is like a rate.
\item When the rate is $\lambda$, we are asking how long do we wait until we first see the bus.
\item Mathematically, this is like calculating $F(t)=1-\lim_{\Delta t\to 0}(1-\lambda\Delta t)^{\frac t{\Delta t}}=1-e^{-\lambda t}$.
\item So we have $f(t)=\lambda e^{-\lambda t}$. Mean value is shown to be $\frac 1\lambda$
\begin{example}
As an exercise, let us check the distribution of $x_{(1)}$ from $n$ samples.
We have $P(x_{(1)}\geq t)=P(x_i\geq t,i=1,2,\dots,n)=e^{-\lambda nt}$, so that $P(x_{(1)}\leq t)=1-e^{-\lambda nt}$.
\end{example}
\end{itemize}
\subsection{Poisson distribution}
\begin{itemize}
\item It is given by $P(X=k)= \lim\limits_{n\to\infty}\binom{n}{k}(\frac{\lambda}n)^k(1-\frac{\lambda}n)^{n-k}$
\item So exponential distribution is about time that we first see an event happening with the rate $\lambda$(essentially success probability over unit time), and poisson distribution is about finding out for a fixed time when the 
rate is $\lambda$, the probability of having $k$ events.
\end{itemize}
\subsection{Beta distribution}
\begin{itemize}
\item It is given by $\frac 1{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}$ where $B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$
\item It can be conjugate prior with respect to Bernoulli by the following formula:
$$
p(\theta\mid x)\propto p^{\sum x_i} (1-p)^{n-\sum x_i} \times p^{\alpha}(1-p)^{n-\alpha}\sim B(\sum x_i + \alpha,\sum (1-x_i)+\beta)
$$
\end{itemize}

\bigsection{Linear Model}
\begin{itemize}
\item Two assumptions: normality($Y_i\sim N(\beta_0 + \beta_1X_i,\sigma^2)$) and moment assumption.
\begin{fact}
For Normal distribution, pair-wise independence implies mutual independence.
\end{fact}
\begin{proof}
It comes from the fact that the mutual independence holds if and only if density function factorises.
In particular, multivariate normal distribution with diagonal matrix covariate satisfies this property.
\end{proof}
\item For 1d case, we could calculate $\hat\beta$ based on $\partial/\partial \beta (RSS)$...
\item After computing $\beta_0$ and $\beta_1$, we can also compute their respective variance and covariance between them.
I need to use the fact that $\bar Y$ and $\hat\beta_1$, which could also be easily calculated.
\item We could further define $\hat \epsilon$, and unbiased estimator $\hat\sigma$. Showing that it is unbiased and is independent to $\hat\beta_0$ and $\hat\beta_1$ is more involved (need to check).
\item Knowing that $\frac{\hat\beta_1-\beta_1}{\sigma/\sqrt{\sum_i(X_i-\bar X)^2}}\sim N(0,1)\perp\text{ (because $\hat\beta_1\perp \hat\sigma^2$) } (n-2)\cdot\frac{\hat\sigma^2}{\sigma^2}\sim\chi^2_{n-2}$
we can do some more analysis involving confidence interval of predictive value and mean value at a given point.
\item Some \textbf{intuition} about why the variance of $\beta$ is inversely proportional to dispersion: well, as the data gets concentrated, the prediction is
rather \textbf{unreliable}. 
\item The reason we used t-distribution is to cancel out the \textbf{unknown value} $\sigma$.
\begin{itemize}
\item RSS = portion that is not explained by the model
\item TSS = total sum of squares = $\lVert Y - \bar Y 1_n\rVert^2$. Note that $Y - \bar Y 1_n\in span(X)$
\item ESS = explained sum of squares = $\lVert \hat Y - \bar Y 1_n\rVert^2$
\end{itemize}
\item RSS + ESS = TSS. The role of $\bar y$ is like the ``standardized line." Refer to \href{https://stats.stackexchange.com/questions/234850/intuition-behind-regression-sum-of-squares}{here}.
\item R square is ESS/TSS.
\item We would use F test... F statistic is given by $F=\frac{\lVert U_2^T Y\rVert^2/k}{\lVert U_3^T Y\rVert^2/(n-p)}=\frac{RSS_\text{partial model}-RSS_\text{full model}}{RSS_\text{full model}/(n-p)}$
\item We want F to be as small as possible for the hypothesis $\beta=0$ to be true, to accept partial model. In other words, $RSS_\text{partial model}$ tends to be larger if $\beta_S\neq 0$.
\end{itemize}
\maketitle
\begin{sloppypar}

\end{sloppypar}
\end{document}