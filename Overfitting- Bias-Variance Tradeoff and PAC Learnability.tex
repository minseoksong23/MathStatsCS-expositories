\documentclass{article}
\usepackage[hyphens,spaces,obeyspaces]{url}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\graphicspath{ {./images/} }
\usetikzlibrary{shapes}
\usepgfplotslibrary{polar}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{backgrounds}
\pgfplotsset{every axis/.append style={
                    axis x line=middle,    % put the x axis in the middle
                    axis y line=middle,    % put the y axis in the middle
                    axis line style={<->,color=blue}, % arrows on the axis
                    xlabel={$},          % default put x on x-axis
                    ylabel={$},          % default put y on y-axis
            }}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\usepackage[utf8]{inputenc}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{remark}[example]{Remark}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\begin{document}
\title{Overfitting: Bias-Variance Tradeoff and PAC Learnability}
\author{MinSeok Song}
\date{}
\maketitle
Overfitting is a central challenge in machine learning and statistical modeling. This phenomenon can be viewed from multiple perspectives.
\begin{proposition}
For a fixed $x$, assume there exists a distribution for $f_k(x)$, representing a distribution over all training sets. Given that the data arises from the model $Y=f(x)+\epsilon$, where the expected value of $\epsilon$ is 0, we have:
\[E(Y-f_k(x)^2) = \sigma^2 + \text{Bias}(f_k)^2 + \text{Var}(f_k(x))\]
\end{proposition}
\begin{remark}
\begin{itemize}
\item $\sigma$ is an irreducible error: this is the noise inherent in any real-world data collection process, which cannot be removed or reduced.
\item The expected value is taken for a distribution over all training sets.
\end{itemize}
\end{remark}

\begin{proof}
The detailed proof involves algebraic manipulations, available at:
\url{https://stats.stackexchange.com/questions/204115/understanding-bias-variance-tradeoff-derivation/354284#354284}
\end{proof}

\begin{proposition}
Given the Empirical Risk Minimization (ERM) chosen hypothesis $h_S = \text{argmin}_{h\in\mathcal{H}}L_{S}(h)$, the following holds:
\begin{enumerate}
    \item $\mathcal{D}^m\left(S\mid_x:L_{\mathcal{D}, f}(h_S)>\epsilon\right) \leq \mathcal{D}^m\left(\bigcup_{h\in \mathcal{H}_B}\{S\mid_x:L_S(h)=0\}\right)$
    
    \item With the assumption of I.I.D. data:
    \[\mathcal{D}^m\left(S\mid_x:L_{\mathcal{D}, f}(h_S)>\epsilon\right) \leq \lvert \mathcal{H_B}\rvert e^{-\epsilon m} \leq \lvert \mathcal{H}\rvert e^{-\epsilon m}\]
\end{enumerate}
\end{proposition}

\begin{remark}
\begin{itemize}
\item The first inequality is the consequence of the assumption that $L_S(h_S)=0$ for $h_S\in \mathcal{H}_\mathcal{B}$. The book is a bit misleading about its assumption on "realizability." It actually means that the set in the left hand side is a "bad training set" in the sense that there exists $h\in \mathcal{H}_\mathcal{B}$ with $R_S(h)=0$.
\item The second inequality employs the inequality $1-\epsilon \leq e^{-\epsilon}$, which is tight for smaller $\epsilon$, and has an analytic advantage by using exponential.
\item The cardinality of $\mathcal{H}$ is used instead of $\mathcal{H}_B$ because we do not know the size of $\mathcal{H_B}$ a priori.
\end{itemize}
\end{remark}

$\ast$ It is important to distinguish the following two contexts of "realizability."
\begin{enumerate}
\item There exists h such that $L_D(h)=0$.
\item For the set with its ERM $h_S$ having non-zero error, it will always contain a hypothesis with zero empirical error.
\end{enumerate}

\begin{corollary}
For a finite hypothesis class $\mathcal{H}$, if $\delta\in (0,1)$, $\epsilon > 0$, and $m \geq \frac{\log(\lvert\mathcal{H}\rvert/\delta)}{\epsilon}$, then:
\[L_{\mathcal{D}, f}(h_S) \leq \epsilon\]
with probability at least $1-\delta$ over an i.i.d. sample $S$ of size $m$, given the realizability assumption.
\end{corollary}

\begin{remark}
\begin{itemize}
\item A smaller $\epsilon$ or $\delta$ necessitates a larger $m$ ($\epsilon$ has a stronger effect), which makes sense.
\item A larger hypothesis class also increases the value of $m$, demonstrating the problem of overfitting. This can be traced back to the necessity for $L_S(h)$ to hold for every possible $S$. However, it's crucial to note that this represents a worst-case scenario and might not be tight.
\item Note that the finiteness of $\lvert \mathcal{H}\rvert$ is crucial here. We have PAC learnability for a finite $VC$ dimension in genral, though this is only a sufficient condition.
\end{itemize}
\end{remark}

\begin{example}
    Let \( h \) be defined as:
    \[
    h_{(a_1, b_1, a_2, b_2)}(x_1, x_2) = 
    \begin{cases} 
    1 & \text{if } a_1 \leq x_1 \leq b_1 \text{ and } a_2 \leq x_2 \leq b_2 \\
    0 & \text{otherwise}
    \end{cases}
    \]
    Consider the hypothesis class consisting of all axis-aligned rectangles in the plane.
    An algorithm that returns the smallest rectangle enclosing all positive examples in the training set is an ERM. 
    The realizability assumption is crucial here in order to guarantee that 0-labeled training sets are located outside of this rectangle. We can show that for $m\geq \frac{4\log(4/\delta)}\epsilon$, the PAC condition holds. 
    \begin{itemize}
    \item To establish the PAC condition, we cannot directly apply the previous corollary due to our hypothesis class being infinite. Instead, the proof requires a blend of geometric insights and probabilistic bounds. Our approach involves considering \( R(S) \subset R^* \) and the outer rectangles \( R_i \) for \( i = 1, 2, 3, 4 \), each with a measure \( D(R_i) = \frac{\epsilon}{4} \). While employ the exponential bounds, which gives an interpretable formula, and adhering to the realizability assumption; we focus on computing the probability that the squares \( R_i \) do not encompass any data points from the distribution.
    \item This is guaranteed by the fundamental theorem of statistical learning; VC dimension is $4<\infty$.
    \item We can generalize to d-dimensional space: we simply replace 4 with 2d.
    \item For each dimension we find minimum and maximum value to construct the smallest enclosing rectangle, so the algorithm to return such a rectangle that ensures the above accuracy/probability 
    from above takes $O(md)=O(d^2\cdot\frac 1\epsilon\cdot \log(1/\delta))$
    \end{itemize}
\end{example}
\begin{example}
    Consider the ERM rule defined such that whenever we encounter a value of \(1\) in our sample dataset, all subsequent values are assigned \(0\). To establish PAC learnability, assume \(P(x_+) > \epsilon\) and \(x_+ \neq S\). The probability that the learned hypothesis \(h_S\) misclassifies is given by 
    \[ P(L_D(h_S)) \leq (1-\epsilon)^m \]
    Following from this, the sample complexity needed for PAC learning can be bounded as:
    \[ m_\mathcal{H}(\epsilon, \delta) \leq \left\lceil \frac{\log (1/\delta)}{\epsilon} \right\rceil \]
    \end{example}

    \begin{example}
    As for minimizing the risk, we note that for binary classification problem, Bayes classifier is optimal. To show this, we need to prove that $$P(f(X)\neq Y\vert X=x)-P(t(X)\neq Y\vert X=x)\geq 0$$ where $t(x)=1$ if $P(Y=1\vert X=x)\geq 1/2$ and -1 otherwise. We can 
    do this by showing that $$P(f(X)\neq\vert X=x)=(1-2P(Y=1\vert X=x))1_{f(x)=1}+P(Y=1\vert X=x)$$ and $$P(t(x)\neq Y\vert X=x)=min(P(Y=1\vert X=x), P(Y=-1\vert X=x)).$$


    \end{example}
    \subsubsection*{Generalization}

    We propose three key ideas to further generalize the learning paradigm:
    
    \begin{enumerate}
        \item \textbf{Joint Distribution Assumption:} 
        Instead of treating the input \(x\) in isolation with \(x \sim \mathcal{D}\), both the input \(x\) and the output \(y\) are assumed to be drawn from a joint distribution, denoted as \((x,y) \sim \mathcal{D}\).
    
        \item \textbf{Agnostic Learnability:}
        Moving beyond the traditional realizability assumption, we introduce the concept of \textit{agnostic learnability}. The criterion for learnability in this context is:
        \[
        L_\mathcal{D}(h) \leq \min_{h' \in \mathcal{H}} L_\mathcal{D}(h') + \epsilon
        \]
        Here, \(L_\mathcal{D}(h)\) represents the expected loss of hypothesis \(h\) under distribution \(\mathcal{D}\).
    
        \item \textbf{Generalized Loss Function:}
        The loss function for our hypothesis can be extended as:
        \[
        L_\mathcal{D}(h) = \mathbb{E}_{z \sim \mathcal{D}} [l(h, z)]
        \]
        The loss function \(l(h,z)\) in prior discussions was defined as a binary function: \(l(h,z) = 1\) if \(h(x) \neq y\) and \(0\) otherwise. This generalized approach permits a broader range of definitions. The empirical loss for a sample set \(S\) is:
        \[
        L_S(h) = \frac{1}{m} \sum_{i=1}^{m} l(h, z_i)
        \]
        As an exemplar, the quadratic loss function can be expressed as:
        \[
        l(h, (x,y)) = (h(x) - y)^2
        \]
    \end{enumerate}
    
    These ideas enable a more flexible framework, suitable for addressing a diverse set of problems and loss functions in machine learning.

    $\diamond$ To further elaborate on the idea of 'relaxation' we can introduce the concept of uniform convergence to get a sufficient condition for agnostic learnability.

    \begin{proposition}
    If a class $\mathcal{H}$ has the uniform convergence property with a function $m^{UC}_{\mathcal{H}}$ then the class is agnostically
     PAC learnable with the sample complexity $m_{\mathcal{H}}(\epsilon,\delta)\leq m^{UC}_{\mathcal{H}}(\epsilon/2,\delta)$. Furthermore, in that case, the $ERM_{\mathcal{H}}$ paradigm is a successful 
     agnostic PAC learner for $\mathcal{H}$.
    \end{proposition}

\end{document}