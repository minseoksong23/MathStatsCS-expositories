\documentclass[11pt,reqno]{amsart}
\usepackage{graphicx, url}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{enumerate}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{hyperref}


\graphicspath{ {./images/} }
\usetikzlibrary{shapes}
\usepgfplotslibrary{polar}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{backgrounds}
\pgfplotsset{every axis/.append style={
                    axis x line=middle,    % put the x axis in the middle
                    axis y line=middle,    % put the y axis in the middle
                    axis line style={<->,color=blue}, % arrows on the axis
                    xlabel={$},          % default put x on x-axis
                    ylabel={$},          % default put y on y-axis
            }}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\usepackage[utf8]{inputenc}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{remark}[example]{Remark}
\newtheorem{fact}{Fact}

\title{Linear Algebra, Numerics}
\author{MinSeok Song}
\date{}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\begin{document}

\maketitle
\tableofcontents

\section{Determinant}
Begin by visualizing an $n\times n$ matrix as a linear transformation. In mathematics, it's often enlightening to 
view objects not just for what they are, but for the roles they play—in this case, as functions. Through this lens, we can perceive the determinant as a function: it ingests
n column vectors from $\mathbb{R}^n$ present in the matrix and 
produces a real number. But what does this number represent? At its essence, the determinant 
can be seen as an indicator of oriented volume.
  \begin{enumerate}
  \item \textbf{Sign Inversion}: Interchanging rows (or columns) of the matrix inverts the sign of the determinant.
  \item \textbf{Linearity}: The determinant is linear in relation to each column and row.
  \item \textbf{Identity Matrix}: The determinant of the identity matrix is 1.
  \end{enumerate}

  With this understanding, we recognize the inherent logic in the definition
   of the determinant. Moreover, when extended to continuous domains, this understanding paves the way to the concept of the wedge product—an 
   essential tool for generalizing integration, which is fundamentally about calculating the 
   "volume" of more complex structures, often referred to as manifolds.


We define $$f\wedge g=\frac 1{k!l!}A(f\otimes g)$$ where
$$Af=\sum_{\sigma\in S_k}(\text{sgn } \sigma)\sigma f$$ and $$f\otimes g(v_1,\dots,v_{k+l})=f(v_1,\dots,v_k)g(v_{k+1},\dots,v_{k+l}).$$
  It follows that $$(\alpha^1\wedge\cdots\wedge \alpha^k)(v_1,\dots, v_k)=det[\alpha^i(v_j)]$$

  The formulation of $f\wedge g$ is meticulously designed to 
  encapsulate the inherent attributes of the determinant. Specifically:
  \begin{enumerate}
    \item The anticommutative nature is reflected in the property 1.
    \item The linearity is mirrored in property 2.
    \item The normalization constant $\frac{1}{k!l!}$ embodies property 3.
  \end{enumerate}
\begin{remark}
\begin{itemize}
 \item The above characterizations intuitively and rigorously (by simply checking that\\
  $\text{det}(A)\text{det}(B)$ satisfies three characterizations) demonstrate why\\
   $ \text{det}(AB) = \text{det}(A) \times \text{det}(B) $.
 \item From this, we can see that the determinant of orthonormal matrix is $-1$ or $1$, and in turn that the determinant is a multiplication of all singular values by $SVD$.
 \item These singular values represent the extent of stretching in each of the \( n \) directions.
 \item The logarithm function translates multiplication into addition and possesses inherent concavity. As a result, for a positive definite matrix \( A \), \( \log \text{det}(A) \) is concave. A more rigorous justification can be derived by verifying \( g''(t) \leq 0 \) for 
 the function \( g(t) = f(Z + tV) \) where \( Z, V \in S^n \).
\end{itemize}
\begin{fact}
$A^{-1}=\frac{1}{det(A)}adj(A)$, with $adj(A)_{ij}=(-1)^{j+i}det(A_{-j,-i})$
\end{fact}
\begin{proof}
We can simply check that the above three properties hold. The multiplication involves
pairing the rows of $A$ with the columns of $adj(A)$. This is precisely why the adjugate is
formed with transposed cofactors. Further, excluding $(i,j)$ index in the definition enables linearity.
\end{proof}
\end{remark}

\section{Pseudo Inverse, rank}
\begin{definition}
Moore-Penrose pseudo-inverse for a matrix $A\in \mathbb{C}^{m\times n}$ is defined as a matrix $X\in\mathbb{C}^{n\times m}$ satisfying
\begin{itemize}
\item $(AX)^*=AX$
\item $(XA)^* = XA$
\item $XAX=X$
\item $AXA=A$
\end{itemize}
\end{definition}
\begin{itemize}
\item Uniqueness can be seen by computing $SVD$ and inverse each singular value, which is canonical.
\item Geometrically, this is a least squares problem ($L^2$ norm): there exists a unique vector $x$ such that $Ax$ is closest to $b$.
\item There is a generalization using different norm, for example $L^\infty$.
\begin{theorem}[Eckhart-Young]
Let $A=\sum\limits^{rank(A)}_{i=1}\sigma_iu_iv_i^*, \sigma_1\geq\dots\geq\sigma_{rank(A)}>0$.
 Then $\arg\limits_X\min\limits_{rank(X)\leq r}\lVert A-X\rVert_2
 =\sum\limits^r_{i=1}\sigma_iu_iv_i^*$. Furthermore, we have 
\[
\min\limits_{rank(X)\leq r}\lVert A-X\rVert_2=\sigma_{r+1}
\]
\end{theorem}
\begin{proof}
The key of the proof is by exploring the space spanned by the right
singular vectors $v_1,\dots,v_{r+1}$ (and connecting it to kernel 
of B) and deriving contradiction from that (for example by 
dimensionality argument).
\end{proof}
\begin{remark}
Eckart-Young holds for any unitarily invariant norm. Intuitively, unitarily invariant norm is the one that "respects" the decomposition of SVD: proof can be found here: \url{https://cklixx.people.wm.edu/uinorm-note.pdf}
\end{remark}
\begin{fact}
The dimension of row vector and column vector is same.
\end{fact}
\begin{proof}
Decompose $A=BC$ where $B$ (m by k) consists of basis of column 
vectors and $C$ (k by n) consists of coefficient for each column vector of $A$.
 On the other hand, we can view each row of $A$ as linear 
 combination of row of $C$ with $B$ giving the coefficients. 
 This implies that $dim(rowsp(A))\leq dim(colsp(A))$. Apply the same
  principle to $A^T$ and we get the desired equality.
\end{proof}
\begin{fact}
$dim(AB)\leq \min (dim(A), dim(B))$
\end{fact}
\begin{fact}
For Hermitian $A$, we have $\max\lVert\langle Ax,x\rangle\rVert=\lVert A\rVert_2$.
\end{fact}
\item This comes from the fact that for Hermitian matrix, eigenvalue and singular value coincide, so when 
the maximum is achieved, $x$ and $Ax$ will have the same direction.
\end{itemize}

\section{Condition number}
\begin{definition}
The absolute condition number of a problem $f$ is
\[
\kappa(f) = \lim_{{\epsilon \to 0^+}} \sup_{{\lVert\delta x\rVert \leq \epsilon}} \frac{\lVert\delta f(x)\rVert / \lVert f(x)\rVert}{\lVert \delta x\rVert / \lVert x\rVert}.
\]
\end{definition}
\begin{itemize}
\item Consider the invertible matrix $A$. Say $f(x):=A^{-1}x$. The condition number will be $\lVert A^{-1}\rVert\frac{\lVert x\rVert}{\lVert A^{-1}x\rVert}$. To eliminate the dependency, we 
consider the "worst case" so that we get $\lVert A\rVert\lVert A^{-1}\rVert$.
\item important to note that condition number may depend on $x$. Unless $f$ is continous, there is no reason to believe that the condition number is a constant.
\end{itemize}
$\ast$ We may illustrate some relationship with relative error.

\begin{itemize}
\item Assume that $A$ is a nonsingular matrix, and let $0\neq b\in \mathbb{R}^n$.
Assume that $\lVert Ax\rVert\leq\lVert A\rVert\lVert x\rVert$ for all $A\in \mathbb{R}^{n\times n}$ and all $x\in \mathbb{R}^n$.
\item Suppose $(A+\Delta A)(\mathbf{x}+\Delta\mathbf{x})=\mathbf{b}$ and
$\mathbf{\hat{x}}=\mathbf{x}+\Delta\mathbf{x}$. We have that
\begin{equation}
\frac{\lVert\Delta\mathbf{x}\rVert}{\lVert\mathbf{\hat{x}}\rVert}\le
\kappa(A)\dfrac{\lVert\Delta A\rVert}{\lVert A\rVert}. \label{eq:perturb2}
\end{equation}
\item Suppose $(A+\Delta A)(\mathbf{x}+\Delta\mathbf{x})=\mathbf{b}$ and
$\mathbf{\hat{x}}=\mathbf{x}+\Delta\mathbf{x}$. Further, suppose that $\frac{\lVert\Delta A\rVert}
{\rVert A\rVert}<\frac 1{\kappa(A)}$.
We have
\begin{equation}
\frac{\lVert\Delta\mathbf{x}\rVert}{\lVert\mathbf{x}\rVert}\le\frac
{\kappa(A)\dfrac{\lVert\Delta A\rVert}{\lVert A\rVert}}{1-\kappa
(A)\dfrac{\lVert\Delta A\rVert}{\lVert A\rVert}}.
\end{equation}
\item Suppose $(A+\Delta A)\mathbf{\hat{x}}=\mathbf{b}
+\Delta\mathbf{b}$ where $\mathbf{\hat{b}}=\mathbf{b}+\Delta\mathbf{b}
\neq\mathbf{0}$ and $\mathbf{\hat{x}}=\mathbf{x}+\Delta\mathbf{x}
\neq\mathbf{0}$. We have
\begin{equation}
\frac{\lVert\Delta\mathbf{x}\rVert}{\lVert\mathbf{\hat{x}}\rVert}\le
\kappa(A)\left(  \frac{\lVert\Delta A\rVert}{\lVert A\rVert}+\frac
{\lVert\Delta\mathbf{b}\rVert}{\lVert\mathbf{\hat{b}}\rVert}+\frac
{\lVert\Delta A\rVert}{\lVert A\rVert}\frac{\lVert\Delta\mathbf{b}\rVert
}{\lVert\mathbf{\hat{b}}\rVert}\right)  . \label{eq:perturb3}
\end{equation}
\item Suppose $(A+\Delta A)\mathbf{\hat{x}}=\mathbf{b}
+\Delta\mathbf{b}$ where $\mathbf{\hat{b}}=\mathbf{b}+\Delta\mathbf{b}
\neq\mathbf{0}$ and $\mathbf{\hat{x}}=\mathbf{x}+\Delta\mathbf{x}
\neq\mathbf{0}$. Further suppose that $\frac{\lVert\Delta A\rVert}{\rVert A\rVert}<\frac 1{\kappa(A)}$.
We then have that
\begin{equation}
\frac{\lVert\Delta\mathbf{x}\rVert}{\lVert\mathbf{x}\rVert}\le\frac
{\kappa(A)\left(  \dfrac{\lVert\Delta A\rVert}{\lVert A\rVert}+\dfrac
{\lVert\Delta\mathbf{b}\rVert}{\lVert\mathbf{b}\rVert}\right)  }
{1-\kappa(A)\dfrac{\lVert\Delta A\rVert}{\lVert A\rVert}}.
\end{equation}
\item As expected, condition number and the relative error of the matrix control the bound of relative error of $x$. The smaller, the better.
\item The rule of thumb here is fordward error $\lesssim$ condition number $\times$ backward error. 
\item The condition involving $\kappa(A)$ is an upper bound of condition number. 
\end{itemize}

\section{Floating Point Arithmetic}

The representation of floating-point numbers in many modern systems follows the IEEE 754 standard. This standard provides a consistent methodology to represent and manipulate real numbers on digital computing systems.

\begin{definition}
Given a floating point representation, \( x \), of the form:
\[ x: \pm a_1a_2 \dots a_{11}b_1b_2 \dots b_{52} \]
We can interpret \( x \) based on its bit patterns:
\begin{itemize}
    \item For \textbf{normalized} values, when \( a_1, \dots, a_{11} \) are not all 0's nor all 1's:
    \[ x = \pm (1.b_1b_2 \dots b_{52})_2 \times 2^{(a_1a_2 \dots a_{11})_2 - 1023} \]
    
    \item For \textbf{subnormal} values when \( a_1, \dots, a_{11} \) are all 0's:
    \[ x = \pm (0.b_1b_2 \dots b_{52})_2 \times 2^{(a_1a_2 \dots a_{11})_2 - 1022} \]
\end{itemize}
\end{definition}
\begin{enumerate}
\item The usage of subnormal is to prevent underflow.
\item This is an excellent example of engineering design to represent real numbers given a fixed number of bits.
\item The subnormal value is always less than or equalt to $2^{-1022}$, and the normalized value if greater than or equal to that, justifying such a choice.
\end{enumerate}
\begin{definition}
The machine epsilon is the gap between 1 and the next largest floating point number. It is $2^{-52}\approx 10^{-16}$ for the double format.

$\ast$ It follows that $\lvert \text{round}(x)-x\rvert\leq\frac{2^{-52}}2\cdot 2^e\leq \frac{2^{-52}}2\lvert x\rvert$.
\end{definition}
\begin{definition}
Largely, we may use two kinds of error; absolute error, $\epsilon_{abs}=\lVert x-\hat x\rVert$ or $\epsilon_{rel} = \frac{\lVert x-\hat x\rVert}{\lVert x\rVert}$.
\end{definition}
\begin{itemize}
\item One might ask, why would we use $x$ instead of $\hat x$ in the denominator. This is purely due to interpretability (just more natural).
\end{itemize}
\subsection{On fl notation}
How do we round? Well, we do
\[
\text{round}(x)=
\begin{cases} 
x_- & \text{if } d_- < d_+ \text{ or } (d_- = d_+ \text{ and } \text{lsb}(x_-) = 0) \\
x_+ & \text{if } d_+ < d_- \text{ or } (d_+ = d_- \text{ and } \text{lsb}(x_+) = 0)
\end{cases}
\]
\begin{itemize}
\item We can check that when $x\in [N_{min}, N_{max}]$, we have $\epsilon_{rel}\leq \epsilon_{machine}$.
\item Trefethen and Bau generalize this in the case $x=\pm (m/\beta^t)\beta^e$ where $\beta^{t-1}\leq m\leq \beta^t-1$, in which case we have the machine epsilon $\frac 12\beta^{1-t}$ (this is an idealized
 case, since we do not have any restriction on the size of bits).
\end{itemize}
\section{Matrix decomposition}
\subsection{SVD decomposition}
\begin{itemize}
\item The proof is brief and illustrative.
\begin{proof}
\begin{enumerate}
\item Apply spectral theorem on $AA^T$. Then we get $A^T A=V\Lambda V^T
=\sum\limits^n_{i=1}(\sigma_i)^2v_iv_i^T$.
\item It follows that $A^T Av_i=(\sigma_i)^2v_i$.
\item Put $u_i=\dfrac{Av_i}{\sigma_i}$.
\item By construction, we have $U=AV\Sigma^{-1}$.
\end{enumerate}
\end{proof}
\item The choice of $u_i$ comes from the fact that we know it is an 
eigenvector of $AA^T$, so it has to be of form $Av_i$, and $\sigma_i$ is just 
a normalizing factor.
\item Time complexity is $O(mn^2)$ where $n<m$. Truncated SVD where 
for getting the first k singular values takes $O(mnk)$.
\item In practice, we would prefer randomized SVD.
\item Without any duplication of singular value, the corresponding eigenvector is unique upto complex number multiplication... now if
we have any duplicates, it's unique upto rotation. The following illustrates this point.
$$
u_1 \sigma_1 v_1^T+ u_2\sigma_1 v_2^T=
\begin{pmatrix}
u_1 & u_2
\end{pmatrix}
\begin{pmatrix}
\sigma &  \\
 & \sigma
\end{pmatrix}
\begin{pmatrix}
v_1^T \\
v_2^T
\end{pmatrix}
=\sigma 
\begin{pmatrix}
u_1 & u_2
\end{pmatrix}
\begin{pmatrix}
v_1^T \\
v_2^T
\end{pmatrix}
=\sigma 
\begin{pmatrix}
u_1 & u_2
\end{pmatrix}
QQ^T
\begin{pmatrix}
v_1^T \\
v_2^T
\end{pmatrix}
$$
\end{itemize}
\subsection{LU decomposition}
\begin{itemize}
\item We decompose by doing Gaussian elimination, resulting in $A=\Pi LU$.
\item The $\Pi$ factor is due to row swapping. We could also do $\Pi AQ=LU$ where $Q$ corresponds to column swapping.
\item Indeed these swappings are purely due to numerical consideration (roundoff error). In general we do not want pivoting element to be small.
\item When $\Pi$ is an identity matrix, we get $A=LU$.
\end{itemize}
\subsection{LDU decomposition}
\begin{itemize}
\item From $LU$, we make the diagonal elements of L and U all 1's. 
\item This makes it easier for us to take inverse of $LU$. 
\end{itemize}

\subsection{Schur decomposition}
\begin{itemize}
\item Refer to \href{https://en.wikipedia.org/wiki/Schur_complement}{here}.
\item The idea is that we can use "block" LDU decomposition.
\end{itemize}

\subsection{Cholesky factorization}
\begin{itemize}
\item Cholesky factorization gives $A=LL^T$ for Hermitian, positive-definite matrix.
\item This is somewhat analogous to $x=(\sqrt x)^2$.
\item Uniqueness is guaranteed for positive definite matrix. However, there is no uniqueness for positive semidefinite.
\item For example, for $
\begin{pmatrix}
0 & 0 \\ 0 & 1
\end{pmatrix}$, we could have set $L=\begin{pmatrix}
  0 & 0 \\ \cos\theta & \sin\theta
\end{pmatrix}
  $ or $L=\begin{pmatrix}
    0 & 0 \\ \sin\theta & \cos\theta
  \end{pmatrix}$
\end{itemize}
\subsection{QR decomposition}
\begin{itemize}
\item Poor man's SVD
\item In the $A=QR$ decomposition, the matrix Q is an orthogonal matrix, 
and the spanning set of the first k leading columns of K is same as
 the space spanning by columns of $A$.
\item In case $A$ is $m\times n$ dimensional and $A$ has rank $r$, we
 can decompose
\begin{enumerate}
\item $m\times r$, $r\times n$ (this is a reduced form capturing the essence of $A$ in terms of its rank).
\item $m\times n$, $n\times n$ (this is referred to as thin or reduced QR decomposition, derived from the next kind).
\item $m\times m$, $m\times n$ (this is the full QR decomposition).
\end{enumerate}
\item We also have $A\Pi=Q
\begin{pmatrix}
  R_1 & S \\
  0 & 0
\end{pmatrix}
  $ and in addition $A=Q\begin{pmatrix}
    L & 0 \\ 0 & 0
  \end{pmatrix} U^*$ where $U$ and $Q$ are orthogonal, by taking $\begin{pmatrix}
  R_1^* \\ S^*
  \end{pmatrix}=Z\begin{pmatrix}
  R_2 \\ 0
  \end{pmatrix}$
\item The detail is canonical.
\item There are several ways to compute: Gram-Schmidt process, Householder transformations,
 and Givens rotations.
\end{itemize}

\subsection{Jordan Canonical form}
\begin{itemize}
\item We have 
\[
T^{-1}AT=J=
\begin{pmatrix}
  J_1 & & & 0 \\
  & \ddots & & \vdots \\
  & & J_k & \\
  0 & \cdots & & J_n \\
\end{pmatrix}
\] where
\[
J_i = 
\begin{pmatrix}
  \lambda_i & 1 & & & 0 \\
  & \lambda_i & 1 & & \vdots \\
  & & \ddots & \ddots & \\
  & & & \lambda_i & 1 \\
  0 & \cdots & & & \lambda_i \\
\end{pmatrix}
\in \mathbb{C}^{n_i\times n_i}
\]
 called Jordan block.
\item Jordan form is unique upto permutations of the blocks.
\end{itemize}
\subsection{CUR decomposition}
\begin{itemize}
\item Also called interpolative decomposition.
\item The main advantage of this decomposition is interpretability, since it uses the column
 from actual data matrix.
\item First choose k columns and rows, denoted by C and R. Calculate $U=\tilde U^+$, and we can approximate as $A=CUR$.
\item However, this is not suited for sparse matrix.

\end{itemize}

\section{Tensor decomposition}
\section{Error analysis, stability}
\begin{definition}
\begin{enumerate}
\item Backward error is the smallest $\Delta x$ such that $f(x+\Delta x)=y^*$. 
\item Forward error is $y^*-y$.
\item Truncation error is the error incurred by Taylor polynomial. 
\begin{itemize}
\item Some intuition of why Taylor polynomial might not converge: the condition for which the error decreases as dimension increases is  
\[
  \lvert x-x_0\rvert<(n+1)\lvert\frac{f^{(n)}
(\xi_{n-1})}{f^{(n+1)}(\xi_n)}
\]
but this might well limit to zero. One example given frequently is $e^{-1/x^2}$ around $0$ and $0$ on $0$, which grows very slowly 
around $0$ (slower than any polynomial).
\end{itemize}
\item Roundoff error is the error caused by floating point approximation.
\end{enumerate}
\end{definition}
\begin{itemize}
\item Backward error is said to be stable if $\lvert\frac{\Delta x}{x}\rvert$ is small for every input $x$. 
\item This is not practical; what if $y^*$ is not even in the range of $f$?
\item An algorithm is said to be numerically stable (mixed forward-backward stability) if 
for any $x\in X$, the computed $\hat y$ satisfies
\[
\hat y + \Delta y=f(x+\Delta x), \lvert \Delta x\rvert\leq \delta\lvert x\rvert, 
\lvert \Delta y\lvert\leq\epsilon\lvert y\rvert.
\]
for small $\delta, \epsilon>0$.
\item This says that our computed value is almost the right answer for almost the right data.
\item In machine learning framework, we often speak of approximation, integration, and optimization error. 
Approximation error is a distance between true value and an output of hypothesis class.
 Integration error is a distance between the output of hypothesis class and the output of ERM.
  Optimization error is a distance between the output of ERM and the hypothesis from of our choice of algorithm.
\end{itemize}
\section{Norms}
\subsection{Nuclear norm}
\begin{itemize}
\item $\lVert A\rVert_*=\sup_{\sigma_1(Q)\leq 1}\langle Q,A\rangle$: this is like saying that the dual norm of spectral norm is nuclear norm.
\item Using this, we can prove that the nuclear norm is actually a norm.
\end{itemize}
\section{miscellaneous facts}
\begin{fact}
determinant of $A$ is equal to the product of eigenvalues.
\end{fact}
\begin{proof}
Note that $det(A-\lambda I) = \Pi_i (\lambda_i -\lambda)$ and plug in $\lambda = 0$.
\end{proof}
\end{document}
