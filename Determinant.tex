\documentclass{article}
\usepackage[hyphens,spaces,obeyspaces]{url}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{amsfonts}
\usepackage{enumitem}
\graphicspath{ {./images/} }
\usetikzlibrary{shapes}
\usepgfplotslibrary{polar}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{backgrounds}
\pgfplotsset{every axis/.append style={
                    axis x line=middle,    % put the x axis in the middle
                    axis y line=middle,    % put the y axis in the middle
                    axis line style={<->,color=blue}, % arrows on the axis
                    xlabel={$},          % default put x on x-axis
                    ylabel={$},          % default put y on y-axis
            }}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\usepackage[utf8]{inputenc}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{remark}[example]{Remark}

\title{On Determinants}
\author{MinSeok Song}
\date{}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\begin{document}

\maketitle

The aim of this document is to delve deeper into the concept of the determinant of a matrix, enriching our understanding beyond the standard definitions.

Begin by visualizing an $n\times n$ matrix as a linear transformation. In mathematics, it's often enlightening to 
view objects not just for what they are, but for the roles they play—in this case, as functions. Through this lens, we can perceive the determinant as a function: it ingests
n column vectors from $\mathbb{R}^n$ present in the matrix and 
produces a real number. But what does this number represent? At its essence, the determinant 
can be seen as an indicator of oriented volume.
  \begin{enumerate}
  \item \textbf{Sign Inversion}: Interchanging rows (or columns) of the matrix inverts the sign of the determinant.
  \item \textbf{Linearity}: The determinant is linear in relation to each column and row.
  \item \textbf{Identity Matrix}: The determinant of the identity matrix is 1.
  \end{enumerate}

  With this understanding, we recognize the inherent logic in the definition
   of the determinant. Moreover, when extended to continuous domains, this understanding paves the way to the concept of the wedge product—an 
   essential tool for generalizing integration, which is fundamentally about calculating the 
   "volume" of more complex structures, often referred to as manifolds.


We define $$f\wedge g=\frac 1{k!l!}A(f\otimes g)$$ where
$$Af=\sum_{\sigma\in S_k}(\text{sgn } \sigma)\sigma f$$ and $$f\otimes g(v_1,\dots,v_{k+l})=f(v_1,\dots,v_k)g(v_{k+1},\dots,v_{k+l}).$$
  It follows that $$(\alpha^1\wedge\cdots\wedge \alpha^k)(v_1,\dots, v_k)=det[\alpha^i(v_j)]$$

  The formulation of $f\wedge g$ is meticulously designed to 
  encapsulate the inherent attributes of the determinant. Specifically:
  \begin{enumerate}
    \item The anticommutative nature is reflected in the property 1.
    \item The linearity is mirrored in property 2.
    \item The normalization constant $\frac{1}{k!l!}$ embodies property 3.
  \end{enumerate}
\begin{remark}
\begin{itemize}
 \item The above characterizations intuitively and rigorously (by simply checking that $\text{det}(A)\text{det}(B)$ satisfies three characterizations) demonstrate why $ \text{det}(AB) = \text{det}(A) \times \text{det}(B) $.
 \item From this, we can see that the determinant of orthonormal matrix is $-1$ or $1$, and in turn that the determinant is a multiplication of all singular values by $SVD$.
 \item These singular values represent the extent of stretching in each of the \( n \) directions.
 \item The logarithm function translates multiplication into addition and possesses inherent concavity. As a result, for a positive definite matrix \( A \), \( \log \text{det}(A) \) is concave. A more rigorous justification can be derived by verifying \( g''(t) \leq 0 \) for 
 the function \( g(t) = f(Z + tV) \) where \( Z, V \in S^n \).
\end{itemize}
\end{remark}
\begin{definition}
Moore-Penrose pseudo-inverse for a matrix $A\in \mathbb{C}^{m\times n}$ is defined as a matrix $X\in\mathbb{C}^{n\times m}$ satisfying
\begin{itemize}
\item $(AX)^*=AX$
\item $(XA)^* = XA$
\item $XAX=X$
\item $AXA=A$
\end{itemize}
\begin{remark}
\begin{itemize}
\item Uniqueness can be seen by computing $SVD$ and inverse each singular value, which is the most obvious thing to do.
\item Geometrically, this is a least squares problem ($L^2$ norm): there exists a unique vector $x$ such that $Ax$ is closest to $b$.
\item We can use different pseudo-inverse by using different norm, say $L^\infty$ norm.

\end{itemize}
\end{remark}
\end{definition}







\end{document}
